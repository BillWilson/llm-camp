{
 "cells": [
  {
   "cell_type": "code",
   "id": "4cd1eaa8-3424-41ad-9cf2-3e8548712865",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T02:34:22.117550Z",
     "start_time": "2024-08-21T02:34:16.134697Z"
    }
   },
   "source": "pip install python-docx elasticsearch",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\r\n",
      "  Obtaining dependency information for python-docx from https://files.pythonhosted.org/packages/3e/3d/330d9efbdb816d3f60bf2ad92f05e1708e4a1b9abe80461ac3444c83f749/python_docx-1.1.2-py3-none-any.whl.metadata\r\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\r\n",
      "Requirement already satisfied: elasticsearch in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (8.14.0)\r\n",
      "Collecting lxml>=3.1.0 (from python-docx)\r\n",
      "  Obtaining dependency information for lxml>=3.1.0 from https://files.pythonhosted.org/packages/eb/6d/d1f1c5e40c64bf62afd7a3f9b34ce18a586a1cccbf71e783cd0a6d8e8971/lxml-5.3.0-cp312-cp312-macosx_10_9_universal2.whl.metadata\r\n",
      "  Downloading lxml-5.3.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.8 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from python-docx) (4.12.2)\r\n",
      "Requirement already satisfied: elastic-transport<9,>=8.13 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from elasticsearch) (8.13.1)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2.2.2)\r\n",
      "Requirement already satisfied: certifi in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2024.6.2)\r\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m244.3/244.3 kB\u001B[0m \u001B[31m1.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading lxml-5.3.0-cp312-cp312-macosx_10_9_universal2.whl (8.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.2/8.2 MB\u001B[0m \u001B[31m4.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m0m\r\n",
      "\u001B[?25hInstalling collected packages: lxml, python-docx\r\n",
      "Successfully installed lxml-5.3.0 python-docx-1.1.2\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T02:43:07.363994Z",
     "start_time": "2024-08-21T02:43:07.292229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import io\n",
    "\n",
    "import requests\n",
    "import docx"
   ],
   "id": "49c598c720bb8905",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "8180e7e4-b90d-4900-a59b-d22e5d6537c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T02:43:10.311876Z",
     "start_time": "2024-08-21T02:43:10.303872Z"
    }
   },
   "source": [
    "def clean_line(line):\n",
    "    line = line.strip()\n",
    "    line = line.strip('\\uFEFF')\n",
    "    return line\n",
    "\n",
    "def read_faq(file_id):\n",
    "    url = f'https://docs.google.com/document/d/{file_id}/export?format=docx'\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with io.BytesIO(response.content) as f_in:\n",
    "        doc = docx.Document(f_in)\n",
    "\n",
    "    questions = []\n",
    "\n",
    "    question_heading_style = 'heading 2'\n",
    "    section_heading_style = 'heading 1'\n",
    "    \n",
    "    heading_id = ''\n",
    "    section_title = ''\n",
    "    question_title = ''\n",
    "    answer_text_so_far = ''\n",
    "     \n",
    "    for p in doc.paragraphs:\n",
    "        style = p.style.name.lower()\n",
    "        p_text = clean_line(p.text)\n",
    "    \n",
    "        if len(p_text) == 0:\n",
    "            continue\n",
    "    \n",
    "        if style == section_heading_style:\n",
    "            section_title = p_text\n",
    "            continue\n",
    "    \n",
    "        if style == question_heading_style:\n",
    "            answer_text_so_far = answer_text_so_far.strip()\n",
    "            if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "                questions.append({\n",
    "                    'text': answer_text_so_far,\n",
    "                    'section': section_title,\n",
    "                    'question': question_title,\n",
    "                })\n",
    "                answer_text_so_far = ''\n",
    "    \n",
    "            question_title = p_text\n",
    "            continue\n",
    "        \n",
    "        answer_text_so_far += '\\n' + p_text\n",
    "    \n",
    "    answer_text_so_far = answer_text_so_far.strip()\n",
    "    if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "        questions.append({\n",
    "            'text': answer_text_so_far,\n",
    "            'section': section_title,\n",
    "            'question': question_title,\n",
    "        })\n",
    "\n",
    "    return questions"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "7d3c2dd7-f64a-4dc7-a4e3-3e8aadfa720f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T02:44:01.734672Z",
     "start_time": "2024-08-21T02:44:01.731412Z"
    }
   },
   "source": [
    "faq_documents = {\n",
    "    'llm-zoomcamp': '1qZjwHkvP0lXHiE4zdbWyUXSVfmVGzougDD6N37bat3E',\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "f94efe26-05e8-4ae5-a0fa-0a8e16852816",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T04:56:07.501983Z",
     "start_time": "2024-08-21T04:56:05.009072Z"
    }
   },
   "source": [
    "documents = []\n",
    "\n",
    "for course, file_id in faq_documents.items():\n",
    "    print(course)\n",
    "    course_documents = read_faq(file_id)\n",
    "    documents.append({'course': course, 'documents': course_documents})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm-zoomcamp\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "1b21af5c-2f6d-49e7-92e9-ca229e2473b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T04:56:09.948559Z",
     "start_time": "2024-08-21T04:56:09.938844Z"
    }
   },
   "source": "documents",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'course': 'llm-zoomcamp',\n",
       "  'documents': [{'text': 'Yes, but if you want to receive a certificate, you need to submit your project while we’re still accepting submissions.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'I just discovered the course. Can I still join?'},\n",
       "   {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework (while the form is Open) without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Course - I have registered for the [insert-zoomcamp-name]. When can I expect to receive the confirmation email?'},\n",
       "   {'text': 'The zoom link is only published to instructors/presenters/TAs.\\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\\nDon’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'What is the video/zoom link to the stream for the “Office Hours” or live/workshop sessions?'},\n",
       "   {'text': 'Issue: I get the notice that due to traffic, I’m on a waitlist for new signups.\\nAnswer: There was a form to submit our emails to, so Alexey can send it in bulk. If you missed that deadline, just sign up manually (or via request tech demo link) and use the chat to request for free hours for “llm zoomcamp”\\nIssue: I’m a pre-existing user from a different zoomcamp and I’m not awarded the free hours even though I’ve submitted my email in the form.\\nAnswer: Just request it via their chat, after you’ve logged in using your pre-existing account, citing “llm zoomcamp” .',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'SaturnCloud - How do I get access?'},\n",
       "   {'text': 'We get 15 free hours per month, which might be limited to the free tier’s hardware configuration.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'SaturnCloud - How many free hours do we get?'},\n",
       "   {'text': 'This message means you have used all allocated hours. Make sure to set Shutout After in settings. Also, do not leave your notebooks running. If your hours are out, try using Google Colab and Kaggle.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'SaturnCloud - Something went wrong. Max of 15 hours of resource usage per month'},\n",
       "   {'text': 'Check the quota and reset cycle carefully - is the free hours per month or per week? Usually if you change the configuration, the free hours quota might also be adjusted,or it might be billed separately.\\nGoogle Colab\\nKaggle\\nDatabricks (?), so many others.\\nUse GPTs to find out. Some might have restrictions on what you can and cannot install, so be sure to read what is included in a free vs paid tier.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Cloud alternatives with GPU'},\n",
       "   {'text': 'When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. Click on the Jump to your record on the leaderboard link to find your entry.\\nIf you want to see what your Display name is, click on the Edit Course Profile button.\\nFirst field is your nickname/displayed-name, change it if you want to be known as your Slack username or Github username or whatever nickname of your choice, if you want to remain anonymous.\\nUnless you want “Lucid Elbakyan” on your certificate, it is mandatory that you change the second field to your official name as in your identification documents - passport, national ID card, driver’s license, etc. This is the name that is going to appear on your Certificate!',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?'},\n",
       "   {'text': \"No, you can only get a certificate if you finish the course with a “live” cohort.\\nWe don't award certificates for the self-paced mode. The reason is you need to peer-review 3 capstone(s) after submitting your own project.\\nYou can only peer-review projects at the time the course is running; after the form is closed and the peer-review list is compiled.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?'},\n",
       "   {'text': 'Yes, you need to pass the Capstone project to get the certificate. Homework is not mandatory, though it is recommended for reinforcing concepts, and the points awarded count towards your rank on the leaderboard.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'I missed the first homework - can I still get a certificate?'},\n",
       "   {'text': 'This course is being offered for the first time, and things will keep changing until a given module is ready, at which point it shall be announced. Working on the material/homework in advance will be at your own risk, as the final version could be different.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'I was working on next week’s homework/content - why does it keep changing?'},\n",
       "   {'text': 'Summer 2025 (via Alexey).',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'When will the course be offered next?'},\n",
       "   {'text': 'Please check the bookmarks and pinned links, especially DataTalks.Club’s YouTube account.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Are there any lectures/videos? Where are they?'},\n",
       "   {'text': 'Your WSL2 is set to use Y.Y GiB, not all your computer memory. Create .wslconfig file under your Windows user profile directory (C:\\\\Users\\\\YourUsername\\\\.wslconfig) with the desired RAM allocation:\\n[wsl2]\\nmemory=8GB\\nRestart WSL: wsl --shutdown\\nRun the free command to verify the changes. For more details, read this article.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'WSL2 - ResponseError: model requires more system memory (X.X GiB) than is available (Y.Y GiB). My system has more than X.X GiB.'},\n",
       "   {'text': 'You may receive the following error when running the OpenAI chat.completions.create command due to insufficient credits in your OpenAI account:',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenAI: Error when running OpenAI chat.completions.create command'},\n",
       "   {'text': \"RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}\\nThe above errors are related to your OpenAI API account’s quota.\\nThere is no free usage of OpenAI’s API so you will be required to add funds using a credit card (see pay as you go in the OpenAI settings at platform.openai.com). Once added, re-run your python command and you should receive a successful return code.\\nSteps to resolve:\\nAdd credits to your account here (min $5)\\nIn chat.completions.create(model='gpt-4o', …) specify one of the available for you models:\\nYou might need to recreate an API key after adding credits to your account and update it locally.\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenAI: Error: RateLimitError: Error code: 429 -'},\n",
       "   {'text': 'Update openai version from 0.27.0 -> any 1.x version',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': \"OpenAI: Error: 'Cannot import name OpenAI from openai'; How to fix?\"},\n",
       "   {'text': 'Using the Openai API does not cost much, you can recharge from 5 dollars. At least for what I spent on the first unit it was barely 5 cents.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenAI: How much will I have to spend to use the Open AI API?'},\n",
       "   {'text': \"No, you don't have to pay for this service in order to complete the course homeworks, you could use some of the alternatives free from this list posted into the course Github.\\nllm-zoomcamp/01-intro/open-ai-alternatives.md at main · DataTalksClub/llm-zoomcamp (github.com)\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenAI: Do I have to subscribe and pay for Open AI API for this course?'},\n",
       "   {'text': 'If you get this error, it’s likely that elasticsearch doesn’t get enough RAM\\nI specified the RAM size to the configuration (-m 4GB)\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-m 4GB \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nOr give it _less_ RAM:\\nTip for Github Codespace users\\nIf you want to run elasticsearch server in a docker, then it may fail with the command in the documentation.\\nIn that case, you can try inserting this line -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\".\\nThis reduces the resource usage.\\nFull command:\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\n-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nIf it doesn\\'t work, try this:\\nsudo sysctl -w vm.max_map_count=262144\\nAnd give the Java machine inside the container more RAM:\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n--ulimit nofile=65536:65536 \\\\\\n--ulimit memlock=-1:-1 \\\\\\n--memory=4g \\\\\\n--cpus=2 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\n-e \"ES_JAVA_OPTS=-Xms2g -Xmx2g\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nAnother possible solution may be to set the memory_lock to false:\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\n-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\\\\n-e \"bootstrap.memory_lock=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'ElasticSearch: ERROR: Elasticsearch exited unexpectedly'},\n",
       "   {'text': 'Instead of document as used in the course video, use doc',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': \"ElasticSearch: ERROR: Elasticsearch.index() got an unexpected keyword argument 'document'\"},\n",
       "   {'text': 'When you stop the container, the data you previously added to elastic will be gone. To avoid it, we can add volume mapping:\\ndocker volume create elasticsearch_data\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-v elasticsearch_data:/usr/share/elasticsearch/data \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Docker: How do I store data persistently in Elasticsearch?'},\n",
       "   {'text': \"You can store your different API keys in a yaml file that you will add in your .gitignore file. Be careful to never push or share this file.\\nFor example, you can create a new file named “api_keys.yml” in your repository.\\nThen, do not forget to add it in your .gitignore file:\\n#api_keys\\napi_keys.yml\\nYou can now fill your api_keys.yml file:\\nOPENAI_API_KEY: “sk[...]”\\nGROQ_API_KEY: “gqk_[...]”\\nSave your file.\\nYou will need the pyyaml library to load your yaml file, so run this command in your terminal:\\npip install pyyaml\\nNow, open your jupyter notebook.\\nYou can load your yaml file and the associated keys with this code:\\nimport yaml\\n# Open the file\\nwith open('api_keys.yml', 'r') as file:\\n# Load the data from the file\\ndata = yaml.safe_load(file)\\n# Get the API key (Groq example here)\\ngroq_api_key = data['GROQ_API_KEY']\\nNow, you can easily replace the “api_key” value directly with the loaded values without loading your environment variables.\\nAdded by Mélanie Fouesnard\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Authentication: Safe and easy way to store and load API keys'},\n",
       "   {'text': 'Option1: using direnv\\ncreated the .envrc file & added my API key, ran direnv allow in the terminal\\nwas getting an error: \"OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\\nresolution: install dotenv & add the following to a cell in the notebook. You can install dotenv by running: pip install python-dotenv.\\nfrom dotenv import load_dotenv\\nload_dotenv(\\'.envrc\\')\\nOption 2: using Codespaces Secrets\\nLog in to your GitHub account and navigate to Settings > Codespaces\\nThere is a section called secrets where you can create Secrets like OPENAI_API_KEY and select for which repositories the secret is supposed to be available.\\nOnce you set this up, the key will be available in your codespaces session',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Authentication: Why is my OPENAI_API_KEY not found in the jupyter notebook?'},\n",
       "   {'text': 'Prior to using Ollama models in llm-zoomcamp tasks, you need to have ollama installed on your pc and the relevant LLM model downloaded with ollama from https://www.ollama.com\\nTo download ollama for Ubuntu:\\n``` curl -fsSL https://ollama.com/install.sh | sh ```\\nTo download ollama for Mac and Windows, follow the guide on this link:\\nhttps://ollama.com/download/\\nOllama a number of open-source LLMs like:\\nLlama3\\nPhi3\\nMistral and Mixtral\\nGemma\\nQwen\\nYou can explore more models on https://ollama.com/library/\\nTo download a model in Ollama, simply open command prompt and type:\\n``` ollama run model_name ```\\ne.g.\\n``` ollama run phi3 ```\\nIt will automatically download the model and you can use it same way as above for later time.\\nTo use Ollama models for inference and llm-zoomcamp tasks, use the following function:\\nimport ollama\\ndef llm(prompt):\\nresponse = ollama.chat(\\nmodel=\"llama3\",\\nmessages=[{\"role\": \"user\", \"content\": prompt}]\\n)\\nreturn response[\\'message\\'][\\'content\\']\\nFor example, we can use it in the following way:\\nprompt = \"When does the llm-zoomcamp course start?\"\\nanswer = llm(prompt)\\nprint(answer)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenSource: How can I use Ollama open-source models locally on my pc without using any API?'},\n",
       "   {'text': \"The question asks for the number of tokens in gpt-4o model. tiktoken is a python library that can be used to get the number of tokens. You don't need openai api key to to get the number of tokens. You can use the code provided in the question to get the number of tokens.\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': \"OpenSource: I am using Groq, and it doesn't provide a tokenizer library based on my research. How can we estimate the number of OpenAI tokens asked in homework question 6?\"},\n",
       "   {'text': 'You can use any LLM platform for your experiments and your project. Also, the homework is designed in such a way that you don’t need to have access to any paid services and can do it locally. However, you would need to adjust the code for that platform. See their documentation pages.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenSource: Can I use Groq instead of OpenAI?'},\n",
       "   {'text': 'Yes. See module 2 and the open-ai-alternatives.md in module 1 folder.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenSource: Can I use open-source alternatives to OpenAI API?'},\n",
       "   {'text': 'This is likely to be an error when indexing the data. First you need to add the index settings before adding the data to the indices, then you will be good to go applying your filters and query.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Returning Empty list after filtering my query (HW Q3)'},\n",
       "   {'text': 'Answer',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Question'},\n",
       "   {'text': 'Please see the General section or use CTRL+F to search this doc.',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Saturn Cloud issues'},\n",
       "   {'text': 'Of course you should have first added your Github repository in SaturnCloud and the SSH Key in your Github account settings.\\nOnce you are in jupyter notebook from SaturnCloud, open the terminal and write these lines:\\n1- Navigate to Your Project Directory:\\ncd /home/jovyan/my_project\\n2- Configure GitHub Remote to Use SSH:\\ngit remote set-url origin git@github.com:username/repository.git\\n3- Stage, Commit and push your changes:\\ngit add .\\ngit commit -m \"Your commit message\"\\ngit push',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'SaturnCloud: How do you manage the changes from SaturnCloud to your Github repository?'},\n",
       "   {'text': 'Clean out your cache using the following code:\\nfrom transformers import TRANSFORMERS_CACHE\\nprint(TRANSFORMERS_CACHE)\\nimport shutil\\nshutil.rmtree(TRANSFORMERS_CACHE)\\nNote: Make sure to shutdown the notebook and restart the kernel',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'SaturnCloud: How can I clean out the hugging face model cache on a saturn cloud notebook?'},\n",
       "   {'text': 'Yes, you can. Here the step to follow:\\n- Open a bash session in the elasticsearch container\\n```bash\\ndocker exec -it elasticsearch bash\\n```\\n- Add path.repo configuration:\\n```bash\\necho path.repo: [\"/usr/share/elasticsearch/backup\"] >> /usr/share/elasticsearch/config/elasticsearch.yml\\n```\\n- Restart container and verify it was created correctly:\\n```bash\\ndocker restart elasticsearch\\ncurl -X GET \"localhost:9200/_snapshot/my_backup?pretty\"\\n```\\n- Create the snapshot (this is the backup ;) )\\n```bash\\ncurl -X PUT \"localhost:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"indices\": \"your_index_name\",\\n\"ignore_unavailable\": true,\\n\"include_global_state\": false\\n}\\n\\'\\n```\\n- Copy the backup to my machine:\\n```bash\\ndocker cp elasticsearch:/usr/share/elasticsearch/backup /path/to/local\\n```\\n- Now create the new container or use docker-compose just in case you are following the module 2:\\n```bash\\ndocker compose up -d\\n```\\n- Add de path.repo configuration in the new one, same as before:\\n```bash\\ndocker exec -it new_elasticsearch bash\\necho path.repo: [\"/usr/share/elasticsearch/backup\"] >> /usr/share/elasticsearch/config/elasticsearch.yml\\n```\\n- Restart the docker container and copy the snapshot in it:\\n```bash\\ndocker restart new_elasticsearch\\ndocker cp /path/to/local/backup new_elasticsearch:/usr/share/elasticsearch\\n```\\n- Register the Snapshot Repository in the New Container:\\n```bash\\ncurl -X PUT \"localhost:9200/_snapshot/my_backup\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"type\": \"fs\",\\n\"settings\": {\\n\"location\": \"/usr/share/elasticsearch/backup\"\\n}\\n}\\n\\'\\n```\\n- Verify if it exists:\\n```bash\\ncurl -X GET \"localhost:9200/_snapshot/my_backup/snapshot_1?pretty\"\\n```\\n- Restore the snapshot:\\n```bash\\ncurl -X POST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"indices\": \"your_index_name\",\\n\"ignore_unavailable\": true,\\n\"include_global_state\": false\\n}\\n\\'\\n```\\n- Show your indexes:\\n```bash\\ncurl -X GET \"localhost:9200/_cat/indices?v\"\\n```\\n- Extra point: If you want to change the original index name by other when you restore the snapshot:\\n```bash\\ncurl -X POST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore?pretty\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"indices\": \"old_index\",\\n\"ignore_unavailable\": true,\\n\"include_global_state\": false,\\n\"rename_pattern\": \"old_index\",\\n\"rename_replacement\": \"new_index\"\\n}\\n\\'\\n```',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'ElasticSearch: Can I backup and restore my elasticsearch index from one to another docker container?'},\n",
       "   {'text': 'You can limit the amount of memory used in the ElasticSearch container by adding the next line to the environment section of your docker-compose. Choose the amount of your preference, e.g.:\\n- \"ES_JAVA_OPTS=-Xms1g -Xmx1g\"  # Set Java heap size to 1GB\\n- You can limit CPU usage for an Elasticsearch service within a docker-compose.yaml file, you can utilize the resource configuration options available in Docker Compose. This includes cpus to limit the number of CPUs that the container can utilize. You can configure your Elasticsearch section in the docker-compose.yaml to restrict CPU usage:\\nservices:\\nelasticsearch:\\nimage: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\\ncontainer_name: elasticsearch\\nenvironment:\\n- discovery.type=single-node\\n- xpack.security.enabled=false\\nports:\\n- \"9200:9200\"\\n- \"9300:9300\"\\ndeploy:\\nresources:\\nlimits:\\ncpus: \\'1.0\\'  # Limits to 1 CPU\\nreservations:\\ncpus: \\'0.5\\'  # Reserves 0.5 CPUs',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'ElasticSearch: How can I limit the memory used by the ElasticSearch container?'},\n",
       "   {'text': 'You have several ways to inspect the content of a file when you are inside a Docker container.\\nFirst, make sure you ran the docker container interactively using bash:\\ndocker exec -it <container> bash\\nThen, you are able to use bash commands. For this case, I propose two solutions:\\nUse “cat” and the file you want to see the content: cat your_file . This will directly print the content in your terminal.\\nInstall vim or nano using apt get and open the file using vim or nano (this can be more suitable for larger files):\\napt-get install vim\\nvim your_file\\nThen, you can exit your file in vim by pressing ESC then typing “:q” and finally press ENTER\\nAdded by Mélanie Fouesnard',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Docker: How to inspect the content of a file inside a Docker container ?'},\n",
       "   {'text': 'Use the following line instead in mounting the current volume to docker for Q4:\\n`-v \"/${PWD}/ollama_files:/root/.ollama\"`',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Docker: Error: Docker mounted volume adds ;C to end of windows path'},\n",
       "   {'text': 'In Docker Desktop, try to increase the resource.\\nGo to the Dashboard > Settings > Resources. Raise the memory limit to 15GB and swap to 4GB - be generous. Applied and restarted the changes\\nAdded by Dandy Arif Rahman',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Docker: Why does inferring using Phi 3 locally take so long on Macbook Air M1?'},\n",
       "   {'text': 'docker system prune -a',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Docker: How can to clean docker cache?'},\n",
       "   {'text': 'A network connection failure usually causes this error and if you try to repeat the operation immediately it’ll still fail. It’s a temporary error, you should wait for 2 or 3 minutes before attempting to pull the model again. Then some minutes later, the operation will success.\\nAdded by Eduardo Muñoz',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Ollama: “Error: pull model manifest: 503: no healthy upstream” when pulling a model with Ollama'},\n",
       "   {'text': 'To solve this you need to pull one of these models first: https://ollama.com/library . Also check the proper name of the module.\\nAdded by Taras Goriachko\\nOllama: Running Ollama locally on Colab gives error after the llm() line\\nAPIConnectionError: Connection error.\\nIt seems to be running at localhost:11434 however localhost:11434/v1/ gives 404\\nFound a solution in the Medium article and this link:\\nhttps://medium.com/@mauryaanoop3/running-ollama-on-google-colab-free-tier-a-step-by-step-guide-9ef74b1f8f7a\\nhttps://github.com/ollama/ollama/issues/703\\nAdded by Hanaa',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Ollama: Error: NotFoundError: Error code: 404 - {\\'error\\': {\\'message\\': \"model XXX not found, try pulling it first\" …'},\n",
       "   {'text': 'ollama list\\nollama rm [model_name]',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Ollama: How can remove Ollama model?'},\n",
       "   {'text': 'InternalServerError: Error code: 500 - {\\'error\\': {\\'message\\': \\'model requires more system memory (5.6 GiB) than is available (1.5 GiB)\\', \\'type\\': \\'api_error\\', \\'param\\': None, \\'code\\': None}}.\\nRunning elastic search with the docker-compose is the cause of the RAM memory issue. To fix this you need to change the docker-compose.yaml file to limit the RAM usage of elastic search\\nversion: \\'3.8\\'\\nservices:\\nelasticsearch:\\nimage: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\\ncontainer_name: elasticsearch\\nenvironment:\\n- discovery.type=single-node\\n- xpack.security.enabled=false\\n- ES_JAVA_OPTS=-Xms1g -Xmx1g  # change 1\\nports:\\n- \"9200:9200\"\\n- \"9300:9300\"\\ndeploy:\\nresources:\\nlimits:\\nmemory: 2G  # change 2\\nollama:\\nimage: ollama/ollama\\ncontainer_name: ollama\\nvolumes:\\n- ollama:/root/.ollama\\nports:\\n- \"11434:11434\"\\nvolumes:\\nollama:\\nAdded by Zoe Zelkha',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Ollama: Error code 500 InternalServerError'},\n",
       "   {'text': 'Manually set the token as below:\\naccess_token = <your_token>\\nmodel  = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)\\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Mistral AI: Unable to get Mistral-7B-v0.1 access despite accepting terms on HF'},\n",
       "   {'text': 'To solve just install transformers directly from github\\n!pip install git+https://github.com/huggingface/transformers',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': \"Python: Error: ModuleNotFoundError: No module named 'transformers.cache_utils'\"},\n",
       "   {'text': 'To solve just install transformers directly from github\\n!pip install git+https://github.com/huggingface/transformers',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Python: Exception: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 40 column 3'},\n",
       "   {'text': 'pip install protobuf==3.20.1\\nAdded by Ibai Irastorza',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Python: from google.protobuf.pyext import _message / TypeError: bases must be types'},\n",
       "   {'text': '1. search with the model name on hugging face.\\n2. get the transformer used on the model.\\n3. using the transformer, encode the string you want.\\n4. calculate the length of the outputted tensor.\\nThe previous code snippet uses the tokenizer of google/gemma-2b LLM. \\nDon’t forget to make your token secret.\\nAdded by kamal',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'HuggingFace: How to get the number of tokens in a certain string related to a certain model on hugging face?'},\n",
       "   {'text': 'The last version I checked for CUDA was 12.5 using a cloud environment like Saturn Cloud. Then the torch package for python should be on supported for that version of CUDA, is followed by cu121 which means that version of torch supports cuda 12.1. Check this page to find the package and version available for CUDA (remember to search the keyword “cu”\\nIn my case I focused on using a torch==2.3.1 and the last cuda version supported was 12.1 (it works on Saturn Cloud)\\nTo install all the needed packages use this command:\\n!pip install transformers accelerate torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121 --trusted-host download.pytorch.org --index-url https://download.pytorch.org/whl/cu121\\nAnd after that just executed this command:\\n!pip install --upgrade transformers',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'How to run a model using CUDA for GPU usage?'},\n",
       "   {'text': 'Upgrade elasticsearch 7.13.3 to 8.14.0 or any 7.x installation to 8.x. The earlier modules used a docker image of elasticsearch 8.4.3 so the python installation of elasticsearch must also be at least 8.x.\\nOr use the keyword ‘body’ instead of ‘document’\\nFor conda users, if you’re trying to update to elasticsearch 8.x using conda install elasticsearch==8.4.3  but getting a “PackagesNotFoundError\", try this:\\n\\n$ conda config --add channels conda-forge\\n$ conda config --set channel_priority strict\\n$ conda install -c conda-forge elasticsearch==8.4.3',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': \"ElasticSearch: Error: Elasticsearch.index() got an unexpected keyword argument 'document'\"},\n",
       "   {'text': 'This worked for me:',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': \"ElasticSearch: TypeError: Elasticsearch.search() got an unexpected keyword argument 'knn'\"},\n",
       "   {'text': 'Try to running docker container based on first course module like this :\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nAnd don’t forget to forwarding your port 9200 if you’re using github codespace or run locally in vscode',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'ElasticSearch: ConnectionError: Connection error caused by: ConnectionError(Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7c455bb94ac0>: Failed to establish a new connection: [Errno 111] Connection refused)) in elastic search'},\n",
       "   {'text': 'As seen in this video: https://www.youtube.com/watch?v=ptByfB_YcEg&t=102s, we can get scores on obtained hits that are greater than 1 despite having a “cosine” similarity measure in our index settings. We would thus expect scores between -1 and 1. However, in the case of the final query, we have several scores additionned together to provide the final score:\\nThe KNN related score, which is between -1 and 1 (cosine similarity)\\nThe text relevance score:  BM25 algorithm scores which can be any positive number, including above 1. This is a “ranking function which calculates score to represent a document\\'s relevance with respect to query” (source: https://stackoverflow.com/questions/43794749/what-is-bm25-and-why-elasticsearch-chose-this-algorithm-for-scoring-in-version-5).\\nSince we have a “match” filter in our query, this triggers the usage of the BM25 ranking algorithm and the final score contains this information.\\nTo get more details about the final scores, you can modify the search query and add an “explain” parameter:\\nresponse = es_client.search(\\nindex=index_name,\\nquery={\\n\"match\": {\"section\": \"General course-related questions\"},\\n},\\nknn=knn_query,\\nsize=5,\\nexplain=True\\n)\\nAdded by Mélanie Fouesnard',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'Why do I get scores greater than 1 on my hits after querying my ElasticSearch database ?'},\n",
       "   {'text': 'For this module homework make sure you install the package sentence-transformers it can be installed as simply as:\\npip install sentence-transformers',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'Not module named “sentence_transformers”'},\n",
       "   {'text': 'I was getting this error at this step: es_client.indices.create(index=index_name, body=index_settings)\\nI checked the log of the elasticsearch server and running this command, the status was red: curl -X GET \"http://localhost:9200/_cluster/health?pretty\"\\nMy problem was that I did not have enough disk space in my computer for docker images. I ended up removing unused ones, manually and pruning:\\ndocker image prune\\ndocker volume prune\\ndocker container prune\\nAdded by Ibai Irastorza',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'Can not create the index: Connection timeout.'},\n",
       "   {'text': 'Make sure your search function receives a query vector, not a dictionary. To resolve this, ensure that the q passed to the search_function within evaluate is correctly transformed into an embedding vector. The following code can help:\\nv_query = embedding_model.encode(query_text)\\nresults = search_function(v_query)',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': \"TypeError: unsupported operand type(s) for *: 'float' and 'dict' when running the vector search function within the evaluate function\"},\n",
       "   {'text': 'max_value = numpy_array.max()',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'Find maximum of an numpy array (of any dimension):'},\n",
       "   {'text': 'Cosine similarity is a measure used to calculate the similarity between two non-zero vectors, often used in text analysis to determine how similar two documents are based on their content. This metric computes the cosine of the angle between two vectors, which are typically word counts or TF-IDF values of the documents. The cosine similarity value ranges from -1 to 1, where 1 indicates that the vectors are identical, 0 indicates that the vectors are orthogonal (no similarity), and -1 represents completely opposite vectors.',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'What is the cosine similarity?'},\n",
       "   {'text': 'A “document” is a collection of fields, which are the key-value pairs that contain your data, that have been serialized as a JSON object.',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'What are documents in ElasticSearch?'},\n",
       "   {'text': 'docker stop elasticsearch\\ndocker rm elasticsearch\\nHow to scale Elastic search scores from [0, 1] to [-1, 1] to compare its results with your own ones, example calculating ranks using dot_product metric ?\\nscore = (es_score - 0.5) * 2',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'runing docker docker: Error response from daemon: Conflict. The container name \"/elasticsearch\" is already in use by container \"20467e6723d78ff2e4e9e0c9a8b9580c07f070e4c852d12c585b1d71aefd6665\". You have to remove (or rename) that container to be able to reuse that name. See \\'docker run --help\\'.'},\n",
       "   {'text': 'Upgrade `sentence-transformers` to v3.0.0>= e.x pip install sentence-transformers>=3.0.0 to avoid the warnings',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'Warning: \\'model \"multi-qa-mpnet-base-dot-v1\" was made on sentence transformers v3.0.0 bet\\' how to suppress?'},\n",
       "   {'text': 'Solution 1 : Install Visual C++ Redistributable\\nSolution 2 : Install Visual Studio, not Visual Studio Code. Like in this depicted below and restart your system. For more details, please follow this link : https://discuss.pytorch.org/t/failed-to-import-pytorch-fbgemm-dll-or-one-of-its-dependencies-is-missing/201969',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'In Windows OS : OSError: [WinError 126] The specified module could not be found. Error loading \"C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\torch\\\\lib\\\\fbgemm.dll\" or one of its dependencies.'},\n",
       "   {'text': 'Inside .env file change POSTGRES_HOST=localhost',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'OperationalError when running python prep.pypsycopg2. OperationalError: could not translate host name \"postgres\" to address: No such host is known. How do I fix this issue?'},\n",
       "   {'text': 'By default, in the dataframe visualization, Pandas truncate the text content in a column to 50 characters. In order to view the entire explanation given by the judge llm for a NON RELEVANT answer, as in figure:\\nThe instruction to show the results must be preceded by:\\npd.set_option(\\'display.max_colwidth\\', None)\\nHere are the specs for the display_max_colwidth option, as describide in the official docs:\\ndisplay.max_colwidth : int or None\\nThe maximum width in characters of a column in the repr of\\na pandas data structure. When the column overflows, a \"...\"\\nplaceholder is embedded in the output. A \\'None\\' value means unlimited.\\n[default: 50] [currently: 50]',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'How set Pandas to show entire text content in a column. Useful to view the entire Explanation column content in the LLM-as-judge section of the offline-rag-evaluation notebook'},\n",
       "   {'text': 'import numpy as np\\nnormalize_vec = lambda v: v / np.linalg.norm(v)\\ndf[\"new_col\"] = df[\"org_col\"].apply(norm_vec)',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'How to normalize vectors in a Pandas DataFrame column (or Pandas Series)?'},\n",
       "   {'text': 'To compute the 75% percentile or 0.75 quantile:\\nquantile: int = df[\"col\"].quantile(q=0.75)',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'How to compute the quantile or percentile of Pandas DataFrame column (or Pandas Series)?'},\n",
       "   {'text': '1. Delete all containers (including running ones):\\n```\\ndocker rm -f\\n```\\n2. Remove all images:\\n```\\ndocker rmi -f\\n```\\n3. Delete all volumes:\\n```\\ndocker volume rm\\n```',\n",
       "    'section': 'Module 5: X',\n",
       "    'question': 'How can I remove all Docker containers, images, and volumes, and builds from the terminal?'},\n",
       "   {'text': 'Use the service name and port provided in the docker-compose.yaml file for the elasticsearch, e.g <http://><docker-compose-service-name>:<port> <http://elasticsearch:9200>',\n",
       "    'section': 'Module 5: X',\n",
       "    'question': \"I have reached the orchestration pipeline's export and I’m facing a connection error at the stage of exporting to the vector database. Can someone help with the connection string?\"},\n",
       "   {'text': 'Answer', 'section': 'Module 6: X', 'question': 'Question'},\n",
       "   {'text': 'Answer', 'section': 'Module 6: X', 'question': 'Question'},\n",
       "   {'text': 'Answer', 'section': 'Capstone Project', 'question': 'Question'},\n",
       "   {'text': 'No, the capstone is a solo project.',\n",
       "    'section': 'Capstone Project',\n",
       "    'question': 'Is it a group project?'},\n",
       "   {'text': 'You only need to submit 1 project. \\nIf the submission at the first attempt fails, you can improve it and re-submit during attempt#2 submission window.\\nIf you want to submit 2 projects for the experience and exposure, you must use different datasets and problem statements.\\nIf you can’t make it to the attempt#1 submission window, you still have time to catch up to meet the attempt#2 submission window\\nRemember that the submission does not count towards the certification if you do not participate in the peer-review of 3 peers in your cohort',\n",
       "    'section': 'Capstone Project',\n",
       "    'question': 'Do we submit 2 projects, what does attempt 1 and 2 mean?'},\n",
       "   {'text': 'No, it does not (answered in office hours Jul 1st, 2024). You can participate in the math-kaggle-llm-competition as a group if you want to form teams; but capstone is an individual attempt.',\n",
       "    'section': 'Capstone Project',\n",
       "    'question': 'Does the competition count as the capstone?'},\n",
       "   {'text': 'Each submitted project will be evaluated by 3 (three) randomly assigned students who have also submitted the project.\\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here (TBA for link).',\n",
       "    'section': 'Capstone Project',\n",
       "    'question': 'How is my capstone project going to be evaluated?'},\n",
       "   {'text': 'Answer: No, you don’t have to use ElasticSearch. You can use any library you want. Just make sure it is documented so your peer-reviewers can reproduce your project.',\n",
       "    'section': 'Certificates',\n",
       "    'question': 'Do I have to use ElasticSearch or X library?'},\n",
       "   {'text': 'Answer', 'section': 'Workshops: dlthub', 'question': 'Question'},\n",
       "   {'text': 'Since dlt is open-source, we can use the content of this workshop for a capstone project. Since the main goal of dlt is to load and store data easily, we can even use it for other zoomcamps (mlops zoomcamp project for example). Do not hesitate to ask questions or use it directly in your projects.\\nAdded by Mélanie Fouesnard',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'Can I use the workshop materials for my own projects or share them with others?'},\n",
       "   {'text': 'The error indicates that you have not changed all instances of “employee_handbook” to “homework” in your pipeline settings',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'There is an error when opening the table using dbtable = db.open_table(\"notion_pages___homework\"): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)'},\n",
       "   {'text': 'Make sure you open the correct table in line 3: dbtable = db.open_table(\"notion_pages___homework\")',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'There is an error when running main(): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)'},\n",
       "   {'text': 'You can use the db.table_names() to list all the tables in the db',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'How do I know which tables are in the db'},\n",
       "   {'text': 'Currently, DLT does not have connectors for ClickHouse or StarRocks but are open to contributions from the community to add these connectors.',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'Does DLT have connectors to ClickHouse or StarRocks?'},\n",
       "   {'text': 'If you get this error\\nOr 401 Client Error , then you either need to grant access to the key or the key is wrong.',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'Notebook does not have secret access or 401 Client Error: Unauthorized for url: https://api.notion.com/v1/search'},\n",
       "   {'text': 'Install directly from source E.g `pip install \"requests @ https://github.com/psf/requests/archive/refs/tags/v2.32.3.zip\"`',\n",
       "    'section': 'Workshops: X',\n",
       "    'question': 'Error: How to fix requests library only installs v2.28 instead of v2.32 required for lancedb?'},\n",
       "   {'text': 'If you get this error while doing the homework , simply restart the ollama server using nohup y running this line of the notebook !nohup ollama serve > nohup.out 2>&1 &\\nIf you do stop and restart the cell, you will need to rerun the cell containing ollama serve first.\\nAdded by Abiodun Gbadamosi',\n",
       "    'section': 'Workshops: X',\n",
       "    'question': 'Connection refused error on prompting the ollam RAG?'},\n",
       "   {'text': 'Answer', 'section': 'Workshops: X', 'question': 'Question'}]}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T05:16:30.598115Z",
     "start_time": "2024-08-21T05:16:30.594836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import hashlib\n",
    "\n",
    "def generate_document_id(doc):\n",
    "    combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}\"\n",
    "    hash_object = hashlib.md5(combined.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    document_id = hash_hex[:8]\n",
    "    return document_id"
   ],
   "id": "ce9e614a936cf216",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T05:17:20.045317Z",
     "start_time": "2024-08-21T05:17:20.042104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_documents = []\n",
    "\n",
    "# print(type(documents))\n",
    "for document in documents:\n",
    "    for doc in document['documents']:\n",
    "        doc['course'] = document['course']\n",
    "        # previously we used just \"id\" for document ID\n",
    "        doc['document_id'] = generate_document_id(doc)\n",
    "        processed_documents.append(doc)\n",
    "\n",
    "print(len(processed_documents))"
   ],
   "id": "f49c7475c6b2575",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "86\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T05:39:21.859308Z",
     "start_time": "2024-08-21T05:37:55.802935Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install mage_ai",
   "id": "b35bc3d6c9ffb6f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mage_ai\r\n",
      "  Obtaining dependency information for mage_ai from https://files.pythonhosted.org/packages/fb/f5/5ea9bea4904d027bd07a122422898182caf0ffb2b5508b9b801a472a8197/mage_ai-0.9.73-py3-none-any.whl.metadata\r\n",
      "  Downloading mage_ai-0.9.73-py3-none-any.whl.metadata (23 kB)\r\n",
      "Collecting cachetools (from mage_ai)\r\n",
      "  Obtaining dependency information for cachetools from https://files.pythonhosted.org/packages/a4/07/14f8ad37f2d12a5ce41206c21820d8cb6561b728e51fad4530dff0552a67/cachetools-5.5.0-py3-none-any.whl.metadata\r\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\r\n",
      "Collecting Faker==4.14.0 (from mage_ai)\r\n",
      "  Obtaining dependency information for Faker==4.14.0 from https://files.pythonhosted.org/packages/6e/6c/437383461986cc472b6ee001138ad637f638349e409a8fc21100fccd089d/Faker-4.14.0-py3-none-any.whl.metadata\r\n",
      "  Downloading Faker-4.14.0-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting GitPython==3.1.41 (from mage_ai)\r\n",
      "  Obtaining dependency information for GitPython==3.1.41 from https://files.pythonhosted.org/packages/45/c6/a637a7a11d4619957cb95ca195168759a4502991b1b91c13d3203ffc3748/GitPython-3.1.41-py3-none-any.whl.metadata\r\n",
      "  Downloading GitPython-3.1.41-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting Jinja2==3.1.3 (from mage_ai)\r\n",
      "  Obtaining dependency information for Jinja2==3.1.3 from https://files.pythonhosted.org/packages/30/6d/6de6be2d02603ab56e72997708809e8a5b0fbfee080735109b40a3564843/Jinja2-3.1.3-py3-none-any.whl.metadata\r\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\r\n",
      "Collecting Pillow==10.3.0 (from mage_ai)\r\n",
      "  Obtaining dependency information for Pillow==10.3.0 from https://files.pythonhosted.org/packages/5e/77/4cf407e7b033b4d8e5fcaac295b6e159cf1c70fa105d769f01ea2e1e5eca/pillow-10.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading pillow-10.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.2 kB)\r\n",
      "Collecting PyGithub==1.59.0 (from mage_ai)\r\n",
      "  Obtaining dependency information for PyGithub==1.59.0 from https://files.pythonhosted.org/packages/81/99/df45c40bf862817354b06bb25b45cd01b98a6e2849969926943ecf8ffb20/PyGithub-1.59.0-py3-none-any.whl.metadata\r\n",
      "  Downloading PyGithub-1.59.0-py3-none-any.whl.metadata (2.0 kB)\r\n",
      "Collecting PyJWT==2.6.0 (from mage_ai)\r\n",
      "  Obtaining dependency information for PyJWT==2.6.0 from https://files.pythonhosted.org/packages/40/46/505f0dd53c14096f01922bf93a7abb4e40e29a06f858abbaa791e6954324/PyJWT-2.6.0-py3-none-any.whl.metadata\r\n",
      "  Downloading PyJWT-2.6.0-py3-none-any.whl.metadata (4.0 kB)\r\n",
      "Collecting aiofiles==22.1.0 (from mage_ai)\r\n",
      "  Obtaining dependency information for aiofiles==22.1.0 from https://files.pythonhosted.org/packages/a0/48/d5d1ab7cfe46e573c3694fa1365442a7d7cadc3abb03d8507e58a3755bb2/aiofiles-22.1.0-py3-none-any.whl.metadata\r\n",
      "  Downloading aiofiles-22.1.0-py3-none-any.whl.metadata (8.0 kB)\r\n",
      "Collecting aiohttp==3.10.0 (from mage_ai)\r\n",
      "  Obtaining dependency information for aiohttp==3.10.0 from https://files.pythonhosted.org/packages/2d/0d/04100b62c3ef4459a9cf199b16d2247d400d4bd57ffd1a81acc59f02ac93/aiohttp-3.10.0-cp312-cp312-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading aiohttp-3.10.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.5 kB)\r\n",
      "Collecting alembic>=1.7.5 (from mage_ai)\r\n",
      "  Obtaining dependency information for alembic>=1.7.5 from https://files.pythonhosted.org/packages/df/ed/c884465c33c25451e4a5cd4acad154c29e5341e3214e220e7f3478aa4b0d/alembic-1.13.2-py3-none-any.whl.metadata\r\n",
      "  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\r\n",
      "Collecting bcrypt==4.0.1 (from mage_ai)\r\n",
      "  Obtaining dependency information for bcrypt==4.0.1 from https://files.pythonhosted.org/packages/78/d4/3b2657bd58ef02b23a07729b0df26f21af97169dbd0b5797afa9e97ebb49/bcrypt-4.0.1-cp36-abi3-macosx_10_10_universal2.whl.metadata\r\n",
      "  Downloading bcrypt-4.0.1-cp36-abi3-macosx_10_10_universal2.whl.metadata (9.0 kB)\r\n",
      "Collecting croniter==1.3.7 (from mage_ai)\r\n",
      "  Obtaining dependency information for croniter==1.3.7 from https://files.pythonhosted.org/packages/80/12/4cf1fa220d521702db5b4e3a06603cac89ecd233ad69b050aa98f08e52ce/croniter-1.3.7-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading croniter-1.3.7-py2.py3-none-any.whl.metadata (22 kB)\r\n",
      "Collecting cryptography==41.0.6 (from mage_ai)\r\n",
      "  Obtaining dependency information for cryptography==41.0.6 from https://files.pythonhosted.org/packages/75/58/2619a79e6f0c986bac0171fa979f078e2389cdad58973507b8fb26bab103/cryptography-41.0.6-cp37-abi3-macosx_10_12_universal2.whl.metadata\r\n",
      "  Downloading cryptography-41.0.6-cp37-abi3-macosx_10_12_universal2.whl.metadata (5.2 kB)\r\n",
      "Collecting dask>=2022.2.0 (from mage_ai)\r\n",
      "  Obtaining dependency information for dask>=2022.2.0 from https://files.pythonhosted.org/packages/a3/f8/b69157baa34fb62ea61a7a1f1cf12c9ef0358f465243a4bc873082a129be/dask-2024.8.1-py3-none-any.whl.metadata\r\n",
      "  Downloading dask-2024.8.1-py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Collecting datadog==0.44.0 (from mage_ai)\r\n",
      "  Obtaining dependency information for datadog==0.44.0 from https://files.pythonhosted.org/packages/28/c5/9699efaf6e530b879fdb5ec2373c5959f396c29a00d4a05a9c4cf9f29292/datadog-0.44.0-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading datadog-0.44.0-py2.py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting freezegun==1.2.2 (from mage_ai)\r\n",
      "  Obtaining dependency information for freezegun==1.2.2 from https://files.pythonhosted.org/packages/50/cd/ba1c8319c002727ccfa03049127218d1767232a77219924d03ba170e0601/freezegun-1.2.2-py3-none-any.whl.metadata\r\n",
      "  Downloading freezegun-1.2.2-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting great-expectations==0.18.12 (from mage_ai)\r\n",
      "  Obtaining dependency information for great-expectations==0.18.12 from https://files.pythonhosted.org/packages/6f/c4/7a1aa862ddb3a4bfa91a68f92918c2692b569beae0b0a8a6125d6b90f71a/great_expectations-0.18.12-py3-none-any.whl.metadata\r\n",
      "  Downloading great_expectations-0.18.12-py3-none-any.whl.metadata (8.9 kB)\r\n",
      "Collecting httpx==0.23.1 (from mage_ai)\r\n",
      "  Obtaining dependency information for httpx==0.23.1 from https://files.pythonhosted.org/packages/e1/74/cdce73069e021ad5913451b86c2707b027975cf302016ca557686d87eb41/httpx-0.23.1-py3-none-any.whl.metadata\r\n",
      "  Downloading httpx-0.23.1-py3-none-any.whl.metadata (52 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m52.5/52.5 kB\u001B[0m \u001B[31m952.0 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting inflection==0.5.1 (from mage_ai)\r\n",
      "  Obtaining dependency information for inflection==0.5.1 from https://files.pythonhosted.org/packages/59/91/aa6bde563e0085a02a435aa99b49ef75b0a4b062635e606dab23ce18d720/inflection-0.5.1-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading inflection-0.5.1-py2.py3-none-any.whl.metadata (1.7 kB)\r\n",
      "Collecting ipykernel==6.15.0 (from mage_ai)\r\n",
      "  Obtaining dependency information for ipykernel==6.15.0 from https://files.pythonhosted.org/packages/bf/39/06ede7abb2f066bf6ebb076fefd80a829d254b2e8a7e2994db89ceed21b4/ipykernel-6.15.0-py3-none-any.whl.metadata\r\n",
      "  Downloading ipykernel-6.15.0-py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Collecting ipython==8.10.0 (from mage_ai)\r\n",
      "  Obtaining dependency information for ipython==8.10.0 from https://files.pythonhosted.org/packages/09/db/d641ee07f319002393524b6c5a8b47370520dcb2b6166a0972cfe9398c60/ipython-8.10.0-py3-none-any.whl.metadata\r\n",
      "  Downloading ipython-8.10.0-py3-none-any.whl.metadata (5.7 kB)\r\n",
      "Collecting itsdangerous~=1.1.0 (from mage_ai)\r\n",
      "  Obtaining dependency information for itsdangerous~=1.1.0 from https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl.metadata (3.1 kB)\r\n",
      "Requirement already satisfied: joblib>=1.1.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from mage_ai) (1.4.2)\r\n",
      "Collecting jupyter-server==1.23.5 (from mage_ai)\r\n",
      "  Obtaining dependency information for jupyter-server==1.23.5 from https://files.pythonhosted.org/packages/77/d0/41f8ac982b49456fe93bd0b00efb49c2df1ac2bada8c92c3d4e702f80477/jupyter_server-1.23.5-py3-none-any.whl.metadata\r\n",
      "  Downloading jupyter_server-1.23.5-py3-none-any.whl.metadata (4.4 kB)\r\n",
      "Collecting jupyter-client==7.4.4 (from mage_ai)\r\n",
      "  Obtaining dependency information for jupyter-client==7.4.4 from https://files.pythonhosted.org/packages/7b/31/2d74aee388c124b5b833f99817bf4b57784ea1b1a828a342c052c48a4f64/jupyter_client-7.4.4-py3-none-any.whl.metadata\r\n",
      "  Downloading jupyter_client-7.4.4-py3-none-any.whl.metadata (8.5 kB)\r\n",
      "Collecting ldap3==2.9.1 (from mage_ai)\r\n",
      "  Obtaining dependency information for ldap3==2.9.1 from https://files.pythonhosted.org/packages/4e/f6/71d6ec9f18da0b2201287ce9db6afb1a1f637dedb3f0703409558981c723/ldap3-2.9.1-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading ldap3-2.9.1-py2.py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Collecting memory-profiler (from mage_ai)\r\n",
      "  Obtaining dependency information for memory-profiler from https://files.pythonhosted.org/packages/49/26/aaca612a0634ceede20682e692a6c55e35a94c21ba36b807cc40fe910ae1/memory_profiler-0.61.0-py3-none-any.whl.metadata\r\n",
      "  Downloading memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\r\n",
      "Collecting newrelic==8.8.0 (from mage_ai)\r\n",
      "  Downloading newrelic-8.8.0.tar.gz (919 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m919.8/919.8 kB\u001B[0m \u001B[31m3.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Installing backend dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.22.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from mage_ai) (1.26.4)\r\n",
      "Requirement already satisfied: pandas>=1.3.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from mage_ai) (2.2.2)\r\n",
      "Collecting polars<0.19.2,>=0.18.0 (from mage_ai)\r\n",
      "  Obtaining dependency information for polars<0.19.2,>=0.18.0 from https://files.pythonhosted.org/packages/61/02/6def1454edc4736db8be834817e70edbd30a2d2b80de9bc93079f16d708d/polars-0.19.1-cp38-abi3-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading polars-0.19.1-cp38-abi3-macosx_11_0_arm64.whl.metadata (14 kB)\r\n",
      "Collecting protobuf~=4.21.12 (from mage_ai)\r\n",
      "  Obtaining dependency information for protobuf~=4.21.12 from https://files.pythonhosted.org/packages/05/b6/6e9b82445e3561132a871e38f5601b12749beb5305eaa085d7f0c59728c9/protobuf-4.21.12-cp37-abi3-macosx_10_9_universal2.whl.metadata\r\n",
      "  Downloading protobuf-4.21.12-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\r\n",
      "Collecting psutil==5.9.8 (from mage_ai)\r\n",
      "  Obtaining dependency information for psutil==5.9.8 from https://files.pythonhosted.org/packages/05/33/2d74d588408caedd065c2497bdb5ef83ce6082db01289a1e1147f6639802/psutil-5.9.8-cp38-abi3-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading psutil-5.9.8-cp38-abi3-macosx_11_0_arm64.whl.metadata (21 kB)\r\n",
      "Collecting pyarrow==14.0.1 (from mage_ai)\r\n",
      "  Obtaining dependency information for pyarrow==14.0.1 from https://files.pythonhosted.org/packages/a4/89/ed4a3be452853dee8579c9a73333b779a71bba3471d4c7710358022a1582/pyarrow-14.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading pyarrow-14.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\r\n",
      "Collecting python-dateutil==2.8.2 (from mage_ai)\r\n",
      "  Obtaining dependency information for python-dateutil==2.8.2 from https://files.pythonhosted.org/packages/36/7a/87837f39d0296e723bb9b62bbb257d0355c7f6128853c78955f57342a56d/python_dateutil-2.8.2-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl.metadata (8.2 kB)\r\n",
      "Requirement already satisfied: pytz>=2022.2.1 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from mage_ai) (2024.1)\r\n",
      "Requirement already satisfied: pyyaml~=6.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from mage_ai) (6.0.1)\r\n",
      "Collecting redis~=5.0.1 (from mage_ai)\r\n",
      "  Obtaining dependency information for redis~=5.0.1 from https://files.pythonhosted.org/packages/c5/d1/19a9c76811757684a0f74adc25765c8a901d67f9f6472ac9d57c844a23c8/redis-5.0.8-py3-none-any.whl.metadata\r\n",
      "  Downloading redis-5.0.8-py3-none-any.whl.metadata (9.2 kB)\r\n",
      "Collecting requests~=2.31.0 (from mage_ai)\r\n",
      "  Obtaining dependency information for requests~=2.31.0 from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\r\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\r\n",
      "Collecting ruamel.yaml==0.17.17 (from mage_ai)\r\n",
      "  Obtaining dependency information for ruamel.yaml==0.17.17 from https://files.pythonhosted.org/packages/c7/75/729b63cd0de2316c8bb789ff2c557d9732a5aeb900c5539ae74db41ba562/ruamel.yaml-0.17.17-py3-none-any.whl.metadata\r\n",
      "  Downloading ruamel.yaml-0.17.17-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: scikit-learn>=1.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from mage_ai) (1.5.0)\r\n",
      "Collecting sentry-sdk==1.19.1 (from mage_ai)\r\n",
      "  Obtaining dependency information for sentry-sdk==1.19.1 from https://files.pythonhosted.org/packages/24/dc/f1d34d1fc0e272003be75a3b4d54517ade4a14960c4ebbe7fe4f58c3d39b/sentry_sdk-1.19.1-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading sentry_sdk-1.19.1-py2.py3-none-any.whl.metadata (8.6 kB)\r\n",
      "Collecting simplejson (from mage_ai)\r\n",
      "  Obtaining dependency information for simplejson from https://files.pythonhosted.org/packages/db/44/acd6122201e927451869d45952b9ab1d3025cdb5e61548d286d08fbccc08/simplejson-3.19.3-cp312-cp312-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading simplejson-3.19.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.2 kB)\r\n",
      "Requirement already satisfied: six>=1.15.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from mage_ai) (1.16.0)\r\n",
      "Collecting sqlalchemy<2.0.0,>=1.4.20 (from mage_ai)\r\n",
      "  Obtaining dependency information for sqlalchemy<2.0.0,>=1.4.20 from https://files.pythonhosted.org/packages/75/f8/6e04c212f8b495c9ec324a5079a57e0d97b49324a14c393388cf8fa100bb/SQLAlchemy-1.4.53-cp312-cp312-macosx_10_9_universal2.whl.metadata\r\n",
      "  Downloading SQLAlchemy-1.4.53-cp312-cp312-macosx_10_9_universal2.whl.metadata (10 kB)\r\n",
      "Collecting sqlglot[rs] (from mage_ai)\r\n",
      "  Obtaining dependency information for sqlglot[rs] from https://files.pythonhosted.org/packages/6c/6e/ee658ca20ea29804ea7bc226df2381bbb95dac6a3735fb6218d1657e4d43/sqlglot-25.15.0-py3-none-any.whl.metadata\r\n",
      "  Downloading sqlglot-25.15.0-py3-none-any.whl.metadata (19 kB)\r\n",
      "Collecting terminado==0.17.1 (from mage_ai)\r\n",
      "  Obtaining dependency information for terminado==0.17.1 from https://files.pythonhosted.org/packages/84/a7/c7628d79651b8c8c775d27b374315a825141b5783512e82026fb210dd639/terminado-0.17.1-py3-none-any.whl.metadata\r\n",
      "  Downloading terminado-0.17.1-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Collecting thefuzz[speedup]==0.19.0 (from mage_ai)\r\n",
      "  Obtaining dependency information for thefuzz[speedup]==0.19.0 from https://files.pythonhosted.org/packages/24/7c/2acf47d228b0c0879468b4e2fd15a14eb58bd97897b4bb8a9a7ed47d22f7/thefuzz-0.19.0-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading thefuzz-0.19.0-py2.py3-none-any.whl.metadata (3.9 kB)\r\n",
      "Collecting tornado==6.3.3 (from mage_ai)\r\n",
      "  Obtaining dependency information for tornado==6.3.3 from https://files.pythonhosted.org/packages/e8/52/4775f3e6630bbc3808e678eb2294beeb654040cf45cc2b66cd6efdcf2571/tornado-6.3.3-cp38-abi3-macosx_10_9_universal2.whl.metadata\r\n",
      "  Downloading tornado-6.3.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (2.5 kB)\r\n",
      "Collecting typer[all]==0.9.0 (from mage_ai)\r\n",
      "  Obtaining dependency information for typer[all]==0.9.0 from https://files.pythonhosted.org/packages/bf/0e/c68adf10adda05f28a6ed7b9f4cd7b8e07f641b44af88ba72d9c89e4de7a/typer-0.9.0-py3-none-any.whl.metadata\r\n",
      "  Downloading typer-0.9.0-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting typing-extensions==4.10.0 (from mage_ai)\r\n",
      "  Obtaining dependency information for typing-extensions==4.10.0 from https://files.pythonhosted.org/packages/f9/de/dc04a3ea60b22624b51c703a84bbe0184abcd1d0b9bc8074b5d6b7ab90bb/typing_extensions-4.10.0-py3-none-any.whl.metadata\r\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting watchdog==4.0.0 (from mage_ai)\r\n",
      "  Obtaining dependency information for watchdog==4.0.0 from https://files.pythonhosted.org/packages/d1/eb/b815a1bd33a34d471d68e1f1833360af861a0a29cd5530b8d95fcf7bc6dc/watchdog-4.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading watchdog-4.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (37 kB)\r\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp==3.10.0->mage_ai)\r\n",
      "  Obtaining dependency information for aiohappyeyeballs>=2.3.0 from https://files.pythonhosted.org/packages/18/b6/58ea188899950d759a837f9a58b2aee1d1a380ea4d6211ce9b1823748851/aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata\r\n",
      "  Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp==3.10.0->mage_ai)\r\n",
      "  Obtaining dependency information for aiosignal>=1.1.2 from https://files.pythonhosted.org/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl.metadata\r\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from aiohttp==3.10.0->mage_ai) (23.2.0)\r\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp==3.10.0->mage_ai)\r\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/3f/ab/c543c13824a615955f57e082c8a5ee122d2d5368e80084f2834e6f4feced/frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\r\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp==3.10.0->mage_ai)\r\n",
      "  Obtaining dependency information for multidict<7.0,>=4.5 from https://files.pythonhosted.org/packages/9c/18/9565f32c19d186168731e859692dfbc0e98f66a1dcf9e14d69c02a78b75a/multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\r\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp==3.10.0->mage_ai)\r\n",
      "  Obtaining dependency information for yarl<2.0,>=1.0 from https://files.pythonhosted.org/packages/54/99/ed3c92c38f421ba6e36caf6aa91c34118771d252dce800118fa2f44d7962/yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (31 kB)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from cryptography==41.0.6->mage_ai) (1.16.0)\r\n",
      "Collecting text-unidecode==1.3 (from Faker==4.14.0->mage_ai)\r\n",
      "  Obtaining dependency information for text-unidecode==1.3 from https://files.pythonhosted.org/packages/a6/a5/c0b6468d3824fe3fde30dbb5e1f687b291608f9473681bbf7dabbf5a87d7/text_unidecode-1.3-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython==3.1.41->mage_ai)\r\n",
      "  Obtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/fd/5b/8f0c4a5bb9fd491c277c21eff7ccae71b47d43c4446c9d0c6cff2fe8c2c4/gitdb-4.0.11-py3-none-any.whl.metadata\r\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\r\n",
      "Collecting altair<5.0.0,>=4.2.1 (from great-expectations==0.18.12->mage_ai)\r\n",
      "  Obtaining dependency information for altair<5.0.0,>=4.2.1 from https://files.pythonhosted.org/packages/18/62/47452306e84d4d2e67f9c559380aeb230f5e6ca84fafb428dd36b96a99ba/altair-4.2.2-py3-none-any.whl.metadata\r\n",
      "  Downloading altair-4.2.2-py3-none-any.whl.metadata (13 kB)\r\n",
      "Collecting Click>=7.1.2 (from great-expectations==0.18.12->mage_ai)\r\n",
      "  Obtaining dependency information for Click>=7.1.2 from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\r\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting colorama>=0.4.3 (from great-expectations==0.18.12->mage_ai)\r\n",
      "  Obtaining dependency information for colorama>=0.4.3 from https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\r\n",
      "Requirement already satisfied: ipywidgets>=7.5.1 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from great-expectations==0.18.12->mage_ai) (8.1.3)\r\n",
      "Collecting jsonpatch>=1.22 (from great-expectations==0.18.12->mage_ai)\r\n",
      "  Obtaining dependency information for jsonpatch>=1.22 from https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Requirement already satisfied: jsonschema>=2.5.1 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from great-expectations==0.18.12->mage_ai) (4.22.0)\r\n",
      "Collecting makefun<2,>=1.7.0 (from great-expectations==0.18.12->mage_ai)\r\n",
      "  Obtaining dependency information for makefun<2,>=1.7.0 from https://files.pythonhosted.org/packages/7a/9d/655cbedbe6df11126bae5752ca41bd8cf5180a01c4148577fc547a4c0b14/makefun-1.15.4-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading makefun-1.15.4-py2.py3-none-any.whl.metadata (3.2 kB)\r\n",
      "Collecting marshmallow<4.0.0,>=3.7.1 (from great-expectations==0.18.12->mage_ai)\r\n",
      "  Obtaining dependency information for marshmallow<4.0.0,>=3.7.1 from https://files.pythonhosted.org/packages/3c/78/c1de55eb3311f2c200a8b91724414b8d6f5ae78891c15d9d936ea43c3dba/marshmallow-3.22.0-py3-none-any.whl.metadata\r\n",
      "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\r\n",
      "Requirement already satisfied: mistune>=0.8.4 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from great-expectations==0.18.12->mage_ai) (3.0.2)\r\n",
      "Requirement already satisfied: nbformat>=5.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from great-expectations==0.18.12->mage_ai) (5.10.4)\r\n",
      "Requirement already satisfied: notebook>=6.4.10 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from great-expectations==0.18.12->mage_ai) (7.1.2)\r\n",
      "Requirement already satisfied: packaging in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from great-expectations==0.18.12->mage_ai) (24.1)\r\n",
      "Requirement already satisfied: pydantic>=1.9.2 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from great-expectations==0.18.12->mage_ai) (2.8.0)\r\n",
      "Collecting pyparsing>=2.4 (from great-expectations==0.18.12->mage_ai)\r\n",
      "  Obtaining dependency information for pyparsing>=2.4 from https://files.pythonhosted.org/packages/9d/ea/6d76df31432a0e6fdf81681a895f009a4bb47b3c39036db3e1b528191d52/pyparsing-3.1.2-py3-none-any.whl.metadata\r\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from great-expectations==0.18.12->mage_ai) (1.14.0)\r\n",
      "Requirement already satisfied: tqdm>=4.59.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from great-expectations==0.18.12->mage_ai) (4.66.4)\r\n",
      "Collecting tzlocal>=1.2 (from great-expectations==0.18.12->mage_ai)\r\n",
      "  Obtaining dependency information for tzlocal>=1.2 from https://files.pythonhosted.org/packages/97/3f/c4c51c55ff8487f2e6d0e618dba917e3c3ee2caae6cf0fbb59c9b1876f2e/tzlocal-5.2-py3-none-any.whl.metadata\r\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\r\n",
      "Requirement already satisfied: urllib3>=1.26 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from great-expectations==0.18.12->mage_ai) (2.2.2)\r\n",
      "Requirement already satisfied: certifi in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from httpx==0.23.1->mage_ai) (2024.6.2)\r\n",
      "Collecting httpcore<0.17.0,>=0.15.0 (from httpx==0.23.1->mage_ai)\r\n",
      "  Obtaining dependency information for httpcore<0.17.0,>=0.15.0 from https://files.pythonhosted.org/packages/04/7e/ef97af4623024e8159993b3114ce208de4f677098ae058ec5882a1bf7605/httpcore-0.16.3-py3-none-any.whl.metadata\r\n",
      "  Downloading httpcore-0.16.3-py3-none-any.whl.metadata (16 kB)\r\n",
      "Collecting rfc3986[idna2008]<2,>=1.3 (from httpx==0.23.1->mage_ai)\r\n",
      "  Obtaining dependency information for rfc3986[idna2008]<2,>=1.3 from https://files.pythonhosted.org/packages/c4/e5/63ca2c4edf4e00657584608bee1001302bbf8c5f569340b78304f2f446cb/rfc3986-1.5.0-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Requirement already satisfied: sniffio in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from httpx==0.23.1->mage_ai) (1.3.1)\r\n",
      "Requirement already satisfied: appnope in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from ipykernel==6.15.0->mage_ai) (0.1.4)\r\n",
      "Requirement already satisfied: debugpy>=1.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from ipykernel==6.15.0->mage_ai) (1.8.2)\r\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from ipykernel==6.15.0->mage_ai) (0.1.7)\r\n",
      "Requirement already satisfied: nest-asyncio in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from ipykernel==6.15.0->mage_ai) (1.6.0)\r\n",
      "Requirement already satisfied: pyzmq>=17 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from ipykernel==6.15.0->mage_ai) (26.0.3)\r\n",
      "Requirement already satisfied: traitlets>=5.1.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from ipykernel==6.15.0->mage_ai) (5.14.3)\r\n",
      "Collecting backcall (from ipython==8.10.0->mage_ai)\r\n",
      "  Obtaining dependency information for backcall from https://files.pythonhosted.org/packages/4c/1c/ff6546b6c12603d8dd1070aa3c3d273ad4c07f5771689a7b69a550e8c951/backcall-0.2.0-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading backcall-0.2.0-py2.py3-none-any.whl.metadata (2.0 kB)\r\n",
      "Requirement already satisfied: decorator in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from ipython==8.10.0->mage_ai) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from ipython==8.10.0->mage_ai) (0.19.1)\r\n",
      "Collecting pickleshare (from ipython==8.10.0->mage_ai)\r\n",
      "  Obtaining dependency information for pickleshare from https://files.pythonhosted.org/packages/9a/41/220f49aaea88bc6fa6cba8d05ecf24676326156c23b991e80b3f2fc24c77/pickleshare-0.7.5-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from ipython==8.10.0->mage_ai) (3.0.47)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from ipython==8.10.0->mage_ai) (2.18.0)\r\n",
      "Requirement already satisfied: stack-data in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from ipython==8.10.0->mage_ai) (0.6.3)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from ipython==8.10.0->mage_ai) (4.9.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from Jinja2==3.1.3->mage_ai) (2.1.5)\r\n",
      "Collecting entrypoints (from jupyter-client==7.4.4->mage_ai)\r\n",
      "  Obtaining dependency information for entrypoints from https://files.pythonhosted.org/packages/35/a8/365059bbcd4572cbc41de17fd5b682be5868b218c3c5479071865cab9078/entrypoints-0.4-py3-none-any.whl.metadata\r\n",
      "  Downloading entrypoints-0.4-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from jupyter-client==7.4.4->mage_ai) (5.7.2)\r\n",
      "Collecting anyio<4,>=3.1.0 (from jupyter-server==1.23.5->mage_ai)\r\n",
      "  Obtaining dependency information for anyio<4,>=3.1.0 from https://files.pythonhosted.org/packages/19/24/44299477fe7dcc9cb58d0a57d5a7588d6af2ff403fdd2d47a246c91a3246/anyio-3.7.1-py3-none-any.whl.metadata\r\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\r\n",
      "Requirement already satisfied: argon2-cffi in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from jupyter-server==1.23.5->mage_ai) (23.1.0)\r\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from jupyter-server==1.23.5->mage_ai) (7.16.4)\r\n",
      "Requirement already satisfied: prometheus-client in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from jupyter-server==1.23.5->mage_ai) (0.20.0)\r\n",
      "Requirement already satisfied: Send2Trash in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from jupyter-server==1.23.5->mage_ai) (1.8.3)\r\n",
      "Requirement already satisfied: websocket-client in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from jupyter-server==1.23.5->mage_ai) (1.8.0)\r\n",
      "Collecting pyasn1>=0.4.6 (from ldap3==2.9.1->mage_ai)\r\n",
      "  Obtaining dependency information for pyasn1>=0.4.6 from https://files.pythonhosted.org/packages/23/7e/5f50d07d5e70a2addbccd90ac2950f81d1edd0783630651d9268d7f1db49/pyasn1-0.6.0-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\r\n",
      "Collecting deprecated (from PyGithub==1.59.0->mage_ai)\r\n",
      "  Obtaining dependency information for deprecated from https://files.pythonhosted.org/packages/20/8d/778b7d51b981a96554f29136cd59ca7880bf58094338085bcf2a979a0e6a/Deprecated-1.2.14-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Collecting pyjwt[crypto]>=2.4.0 (from PyGithub==1.59.0->mage_ai)\r\n",
      "  Obtaining dependency information for pyjwt[crypto]>=2.4.0 from https://files.pythonhosted.org/packages/79/84/0fdf9b18ba31d69877bd39c9cd6052b47f3761e9910c15de788e519f079f/PyJWT-2.9.0-py3-none-any.whl.metadata\r\n",
      "  Downloading PyJWT-2.9.0-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting pynacl>=1.4.0 (from PyGithub==1.59.0->mage_ai)\r\n",
      "  Obtaining dependency information for pynacl>=1.4.0 from https://files.pythonhosted.org/packages/ce/75/0b8ede18506041c0bf23ac4d8e2971b4161cd6ce630b177d0a08eb0d8857/PyNaCl-1.5.0-cp36-abi3-macosx_10_10_universal2.whl.metadata\r\n",
      "  Downloading PyNaCl-1.5.0-cp36-abi3-macosx_10_10_universal2.whl.metadata (8.7 kB)\r\n",
      "Requirement already satisfied: ptyprocess in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from terminado==0.17.1->mage_ai) (0.7.0)\r\n",
      "Collecting python-levenshtein>=0.12 (from thefuzz[speedup]==0.19.0->mage_ai)\r\n",
      "  Obtaining dependency information for python-levenshtein>=0.12 from https://files.pythonhosted.org/packages/72/8e/559c539e76bc0b1defec3da39a047fe151258efc9b215bf41db41e2c7922/python_Levenshtein-0.25.1-py3-none-any.whl.metadata\r\n",
      "  Downloading python_Levenshtein-0.25.1-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]==0.9.0->mage_ai)\r\n",
      "  Obtaining dependency information for shellingham<2.0.0,>=1.3.0 from https://files.pythonhosted.org/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\r\n",
      "Collecting rich<14.0.0,>=10.11.0 (from typer[all]==0.9.0->mage_ai)\r\n",
      "  Obtaining dependency information for rich<14.0.0,>=10.11.0 from https://files.pythonhosted.org/packages/87/67/a37f6214d0e9fe57f6ae54b2956d550ca8365857f42a1ce0392bb21d9410/rich-13.7.1-py3-none-any.whl.metadata\r\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\r\n",
      "Collecting Mako (from alembic>=1.7.5->mage_ai)\r\n",
      "  Obtaining dependency information for Mako from https://files.pythonhosted.org/packages/03/62/70f5a0c2dd208f9f3f2f9afd103aec42ee4d9ad2401d78342f75e9b8da36/Mako-1.3.5-py3-none-any.whl.metadata\r\n",
      "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Collecting cloudpickle>=3.0.0 (from dask>=2022.2.0->mage_ai)\r\n",
      "  Obtaining dependency information for cloudpickle>=3.0.0 from https://files.pythonhosted.org/packages/96/43/dae06432d0c4b1dc9e9149ad37b4ca8384cf6eb7700cd9215b177b914f0a/cloudpickle-3.0.0-py3-none-any.whl.metadata\r\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\r\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from dask>=2022.2.0->mage_ai) (2024.6.1)\r\n",
      "Collecting partd>=1.4.0 (from dask>=2022.2.0->mage_ai)\r\n",
      "  Obtaining dependency information for partd>=1.4.0 from https://files.pythonhosted.org/packages/71/e7/40fb618334dcdf7c5a316c0e7343c5cd82d3d866edc100d98e29bc945ecd/partd-1.4.2-py3-none-any.whl.metadata\r\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\r\n",
      "Collecting toolz>=0.10.0 (from dask>=2022.2.0->mage_ai)\r\n",
      "  Obtaining dependency information for toolz>=0.10.0 from https://files.pythonhosted.org/packages/b7/8a/d82202c9f89eab30f9fc05380daae87d617e2ad11571ab23d7c13a29bb54/toolz-0.12.1-py3-none-any.whl.metadata\r\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from pandas>=1.3.0->mage_ai) (2024.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from requests~=2.31.0->mage_ai) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from requests~=2.31.0->mage_ai) (3.7)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from scikit-learn>=1.0->mage_ai) (3.5.0)\r\n",
      "Collecting sqlglotrs==0.2.8 (from sqlglot[rs]->mage_ai)\r\n",
      "  Obtaining dependency information for sqlglotrs==0.2.8 from https://files.pythonhosted.org/packages/62/f0/ffda60c28338e8272b708ee0d6fd9a3ae4bad5bda05514452a396f75caaf/sqlglotrs-0.2.8-cp312-cp312-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading sqlglotrs-0.2.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (258 bytes)\r\n",
      "Requirement already satisfied: pycparser in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from cffi>=1.12->cryptography==41.0.6->mage_ai) (2.22)\r\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython==3.1.41->mage_ai)\r\n",
      "  Obtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\r\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from httpcore<0.17.0,>=0.15.0->httpx==0.23.1->mage_ai) (0.14.0)\r\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from ipywidgets>=7.5.1->great-expectations==0.18.12->mage_ai) (0.2.2)\r\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from ipywidgets>=7.5.1->great-expectations==0.18.12->mage_ai) (4.0.11)\r\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from ipywidgets>=7.5.1->great-expectations==0.18.12->mage_ai) (3.0.11)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from jedi>=0.16->ipython==8.10.0->mage_ai) (0.8.4)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from jsonpatch>=1.22->great-expectations==0.18.12->mage_ai) (3.0.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from jsonschema>=2.5.1->great-expectations==0.18.12->mage_ai) (2023.12.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from jsonschema>=2.5.1->great-expectations==0.18.12->mage_ai) (0.35.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from jsonschema>=2.5.1->great-expectations==0.18.12->mage_ai) (0.18.1)\r\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from jupyter-core>=4.9.2->jupyter-client==7.4.4->mage_ai) (4.2.2)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server==1.23.5->mage_ai) (4.12.3)\r\n",
      "Requirement already satisfied: bleach!=5.0.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server==1.23.5->mage_ai) (6.1.0)\r\n",
      "Requirement already satisfied: defusedxml in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server==1.23.5->mage_ai) (0.7.1)\r\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server==1.23.5->mage_ai) (0.3.0)\r\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server==1.23.5->mage_ai) (0.10.0)\r\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server==1.23.5->mage_ai) (1.5.1)\r\n",
      "Requirement already satisfied: tinycss2 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server==1.23.5->mage_ai) (1.3.0)\r\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from nbformat>=5.0->great-expectations==0.18.12->mage_ai) (2.20.0)\r\n",
      "INFO: pip is looking at multiple versions of notebook to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting notebook>=6.4.10 (from great-expectations==0.18.12->mage_ai)\r\n",
      "  Obtaining dependency information for notebook>=6.4.10 from https://files.pythonhosted.org/packages/32/b4/b0cdaf52c35a3a40633136bee5152d6670acb555c698d23a3458dca65781/notebook-7.2.1-py3-none-any.whl.metadata\r\n",
      "  Downloading notebook-7.2.1-py3-none-any.whl.metadata (10 kB)\r\n",
      "  Obtaining dependency information for notebook>=6.4.10 from https://files.pythonhosted.org/packages/64/76/4437268f47f452fb4cd5cf73fa831241ea8130ae0ab9c64d5c4ffca9f121/notebook-7.2.0-py3-none-any.whl.metadata\r\n",
      "  Downloading notebook-7.2.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "  Obtaining dependency information for notebook>=6.4.10 from https://files.pythonhosted.org/packages/08/e6/e032f6a4d3da85c4bd38b6ff96de51347bfb0d932ea1d2ccfd89c6374c8a/notebook-7.1.3-py3-none-any.whl.metadata\r\n",
      "  Downloading notebook-7.1.3-py3-none-any.whl.metadata (10 kB)\r\n",
      "  Obtaining dependency information for notebook>=6.4.10 from https://files.pythonhosted.org/packages/09/ab/332eaee7872d3883cf3a2cc6a2b2b1e36a566f16fa432c89204a5bd486e6/notebook-7.1.1-py3-none-any.whl.metadata\r\n",
      "  Downloading notebook-7.1.1-py3-none-any.whl.metadata (10 kB)\r\n",
      "  Obtaining dependency information for notebook>=6.4.10 from https://files.pythonhosted.org/packages/5f/38/f5a11c1e68bf3dbd54c7c98f301bf9495e8735803b42ee2f740c5b7c1ca5/notebook-7.1.0-py3-none-any.whl.metadata\r\n",
      "  Downloading notebook-7.1.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "  Obtaining dependency information for notebook>=6.4.10 from https://files.pythonhosted.org/packages/3b/4d/92ac6c0ae6501edd277b8d7bb00bdcb1527a35c2d8467ecfd5cc1405ae93/notebook-7.0.8-py3-none-any.whl.metadata\r\n",
      "  Downloading notebook-7.0.8-py3-none-any.whl.metadata (10 kB)\r\n",
      "  Obtaining dependency information for notebook>=6.4.10 from https://files.pythonhosted.org/packages/f2/57/2f8d59ddc7f2d0d8ac4f80f869545bc44646fc78c1c083b3655c58e3edfb/notebook-7.0.7-py3-none-any.whl.metadata\r\n",
      "  Downloading notebook-7.0.7-py3-none-any.whl.metadata (10 kB)\r\n",
      "INFO: pip is still looking at multiple versions of notebook to determine which version is compatible with other requirements. This could take a while.\r\n",
      "  Obtaining dependency information for notebook>=6.4.10 from https://files.pythonhosted.org/packages/f3/2b/b904c57709b83c6cbd818d21040db36719207f3d17db9b124c60cd483d94/notebook-7.0.6-py3-none-any.whl.metadata\r\n",
      "  Downloading notebook-7.0.6-py3-none-any.whl.metadata (10 kB)\r\n",
      "  Obtaining dependency information for notebook>=6.4.10 from https://files.pythonhosted.org/packages/f8/8e/d7ac71e1d90746a30c2bc34424ec62be08813859aba4c367fc88fe678d68/notebook-7.0.5-py3-none-any.whl.metadata\r\n",
      "  Downloading notebook-7.0.5-py3-none-any.whl.metadata (10 kB)\r\n",
      "  Obtaining dependency information for notebook>=6.4.10 from https://files.pythonhosted.org/packages/29/e0/50b48473fcb99651dd21302da50ae2c49113ccf3dfb901fc6aaa3117e7ed/notebook-7.0.4-py3-none-any.whl.metadata\r\n",
      "  Downloading notebook-7.0.4-py3-none-any.whl.metadata (10 kB)\r\n",
      "  Obtaining dependency information for notebook>=6.4.10 from https://files.pythonhosted.org/packages/79/97/b803f9686fe25b40c8eca2d1ba440b2f878f23b87765d18fe93dd323f0b7/notebook-7.0.3-py3-none-any.whl.metadata\r\n",
      "  Downloading notebook-7.0.3-py3-none-any.whl.metadata (10 kB)\r\n",
      "  Obtaining dependency information for notebook>=6.4.10 from https://files.pythonhosted.org/packages/85/76/51777158391e1784cc5a892ff355fd0a0b0b5189aeffce9baf51f514b9a2/notebook-7.0.2-py3-none-any.whl.metadata\r\n",
      "  Downloading notebook-7.0.2-py3-none-any.whl.metadata (10 kB)\r\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\r\n",
      "  Obtaining dependency information for notebook>=6.4.10 from https://files.pythonhosted.org/packages/c6/3a/171eaf5808d22b97fc98c61f2dd484302d1f38dbe0952e536dbaa88c2320/notebook-7.0.1-py3-none-any.whl.metadata\r\n",
      "  Downloading notebook-7.0.1-py3-none-any.whl.metadata (10 kB)\r\n",
      "  Obtaining dependency information for notebook>=6.4.10 from https://files.pythonhosted.org/packages/b6/7a/9a32d2c8a2652614f66ee889f67c352fa11e1042e668de3224c4da2370db/notebook-7.0.0-py3-none-any.whl.metadata\r\n",
      "  Downloading notebook-7.0.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "  Obtaining dependency information for notebook>=6.4.10 from https://files.pythonhosted.org/packages/af/9c/0620631da9d7013e95a8f985043cad229a0d8fb537a7e3f8ff8467565a8c/notebook-6.5.7-py3-none-any.whl.metadata\r\n",
      "  Downloading notebook-6.5.7-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting ipython-genutils (from notebook>=6.4.10->great-expectations==0.18.12->mage_ai)\r\n",
      "  Obtaining dependency information for ipython-genutils from https://files.pythonhosted.org/packages/fa/bc/9bd3b5c2b4774d5f33b2d544f1460be9df7df2fe42f352135381c347c69a/ipython_genutils-0.2.0-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl.metadata (755 bytes)\r\n",
      "Collecting nbclassic>=0.4.7 (from notebook>=6.4.10->great-expectations==0.18.12->mage_ai)\r\n",
      "  Obtaining dependency information for nbclassic>=0.4.7 from https://files.pythonhosted.org/packages/59/d1/86fcfe1d1e5b66ec61632b014612e2d074a9d2bdf0eed53b90c2536e8dd3/nbclassic-1.1.0-py3-none-any.whl.metadata\r\n",
      "  Downloading nbclassic-1.1.0-py3-none-any.whl.metadata (3.6 kB)\r\n",
      "Collecting locket (from partd>=1.4.0->dask>=2022.2.0->mage_ai)\r\n",
      "  Obtaining dependency information for locket from https://files.pythonhosted.org/packages/db/bc/83e112abc66cd466c6b83f99118035867cecd41802f8d044638aa78a106e/locket-1.0.0-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\r\n",
      "Requirement already satisfied: wcwidth in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython==8.10.0->mage_ai) (0.2.13)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from pydantic>=1.9.2->great-expectations==0.18.12->mage_ai) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.20.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from pydantic>=1.9.2->great-expectations==0.18.12->mage_ai) (2.20.0)\r\n",
      "INFO: pip is looking at multiple versions of pyjwt[crypto] to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting pyjwt[crypto]>=2.4.0 (from PyGithub==1.59.0->mage_ai)\r\n",
      "  Obtaining dependency information for pyjwt[crypto]>=2.4.0 from https://files.pythonhosted.org/packages/2b/4f/e04a8067c7c96c364cef7ef73906504e2f40d690811c021e1a1901473a19/PyJWT-2.8.0-py3-none-any.whl.metadata\r\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\r\n",
      "  Obtaining dependency information for pyjwt[crypto]>=2.4.0 from https://files.pythonhosted.org/packages/c7/e8/01b2e35d81e618a8212e651e10c91660bdfda49c1d15ce66f4ca1ff43649/PyJWT-2.7.0-py3-none-any.whl.metadata\r\n",
      "  Downloading PyJWT-2.7.0-py3-none-any.whl.metadata (4.1 kB)\r\n",
      "Collecting Levenshtein==0.25.1 (from python-levenshtein>=0.12->thefuzz[speedup]==0.19.0->mage_ai)\r\n",
      "  Obtaining dependency information for Levenshtein==0.25.1 from https://files.pythonhosted.org/packages/1b/35/8a20840459669fcb94e8a2630cc5850955af87a550975fa148eda1793e16/Levenshtein-0.25.1-cp312-cp312-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading Levenshtein-0.25.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.3 kB)\r\n",
      "Collecting rapidfuzz<4.0.0,>=3.8.0 (from Levenshtein==0.25.1->python-levenshtein>=0.12->thefuzz[speedup]==0.19.0->mage_ai)\r\n",
      "  Obtaining dependency information for rapidfuzz<4.0.0,>=3.8.0 from https://files.pythonhosted.org/packages/22/a5/8c14e41bcdea3be343764de6ad464ff87300352f8e2d01064bf0f1809e9d/rapidfuzz-3.9.6-cp312-cp312-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading rapidfuzz-3.9.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\r\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14.0.0,>=10.11.0->typer[all]==0.9.0->mage_ai)\r\n",
      "  Obtaining dependency information for markdown-it-py>=2.2.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata\r\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\r\n",
      "Requirement already satisfied: argon2-cffi-bindings in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from argon2-cffi->jupyter-server==1.23.5->mage_ai) (21.2.0)\r\n",
      "Collecting wrapt<2,>=1.10 (from deprecated->PyGithub==1.59.0->mage_ai)\r\n",
      "  Obtaining dependency information for wrapt<2,>=1.10 from https://files.pythonhosted.org/packages/6a/d7/cfcd73e8f4858079ac59d9db1ec5a1349bc486ae8e9ba55698cc1f4a1dff/wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from stack-data->ipython==8.10.0->mage_ai) (2.0.1)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from stack-data->ipython==8.10.0->mage_ai) (2.4.1)\r\n",
      "Requirement already satisfied: pure-eval in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from stack-data->ipython==8.10.0->mage_ai) (0.2.2)\r\n",
      "Requirement already satisfied: webencodings in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server==1.23.5->mage_ai) (0.5.1)\r\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]==0.9.0->mage_ai)\r\n",
      "  Obtaining dependency information for mdurl~=0.1 from https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl.metadata\r\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: notebook-shim>=0.2.3 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from nbclassic>=0.4.7->notebook>=6.4.10->great-expectations==0.18.12->mage_ai) (0.2.4)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/billhsieh/work/llm-camp/venv/lib/python3.12/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server==1.23.5->mage_ai) (2.5)\r\n",
      "Downloading mage_ai-0.9.73-py3-none-any.whl (39.9 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m39.9/39.9 MB\u001B[0m \u001B[31m5.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading aiofiles-22.1.0-py3-none-any.whl (14 kB)\r\n",
      "Downloading aiohttp-3.10.0-cp312-cp312-macosx_11_0_arm64.whl (385 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m385.3/385.3 kB\u001B[0m \u001B[31m9.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading bcrypt-4.0.1-cp36-abi3-macosx_10_10_universal2.whl (473 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m473.4/473.4 kB\u001B[0m \u001B[31m23.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading croniter-1.3.7-py2.py3-none-any.whl (17 kB)\r\n",
      "Downloading cryptography-41.0.6-cp37-abi3-macosx_10_12_universal2.whl (5.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.3/5.3 MB\u001B[0m \u001B[31m23.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading datadog-0.44.0-py2.py3-none-any.whl (111 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m111.0/111.0 kB\u001B[0m \u001B[31m16.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading Faker-4.14.0-py3-none-any.whl (1.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading freezegun-1.2.2-py3-none-any.whl (17 kB)\r\n",
      "Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m196.4/196.4 kB\u001B[0m \u001B[31m10.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading great_expectations-0.18.12-py3-none-any.whl (5.4 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.4/5.4 MB\u001B[0m \u001B[31m18.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading httpx-0.23.1-py3-none-any.whl (84 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m85.0/85.0 kB\u001B[0m \u001B[31m7.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\r\n",
      "Downloading ipykernel-6.15.0-py3-none-any.whl (133 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m133.1/133.1 kB\u001B[0m \u001B[31m4.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading ipython-8.10.0-py3-none-any.whl (784 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m784.3/784.3 kB\u001B[0m \u001B[31m13.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m133.2/133.2 kB\u001B[0m \u001B[31m13.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading jupyter_client-7.4.4-py3-none-any.whl (132 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m132.3/132.3 kB\u001B[0m \u001B[31m3.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading jupyter_server-1.23.5-py3-none-any.whl (346 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m347.0/347.0 kB\u001B[0m \u001B[31m11.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading ldap3-2.9.1-py2.py3-none-any.whl (432 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m432.2/432.2 kB\u001B[0m \u001B[31m13.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading pillow-10.3.0-cp312-cp312-macosx_11_0_arm64.whl (3.4 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.4/3.4 MB\u001B[0m \u001B[31m21.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading psutil-5.9.8-cp38-abi3-macosx_11_0_arm64.whl (249 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m249.9/249.9 kB\u001B[0m \u001B[31m13.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading pyarrow-14.0.1-cp312-cp312-macosx_11_0_arm64.whl (24.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.0/24.0 MB\u001B[0m \u001B[31m5.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading PyGithub-1.59.0-py3-none-any.whl (342 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m342.1/342.1 kB\u001B[0m \u001B[31m20.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\r\n",
      "Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m247.7/247.7 kB\u001B[0m \u001B[31m18.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading ruamel.yaml-0.17.17-py3-none-any.whl (109 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m109.1/109.1 kB\u001B[0m \u001B[31m10.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading sentry_sdk-1.19.1-py2.py3-none-any.whl (199 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m199.2/199.2 kB\u001B[0m \u001B[31m11.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading terminado-0.17.1-py3-none-any.whl (17 kB)\r\n",
      "Downloading tornado-6.3.3-cp38-abi3-macosx_10_9_universal2.whl (425 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m425.4/425.4 kB\u001B[0m \u001B[31m18.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\r\n",
      "Downloading watchdog-4.0.0-cp312-cp312-macosx_11_0_arm64.whl (93 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m93.2/93.2 kB\u001B[0m \u001B[31m8.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.2/78.2 kB\u001B[0m \u001B[31m8.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m233.0/233.0 kB\u001B[0m \u001B[31m11.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading dask-2024.8.1-py3-none-any.whl (1.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m24.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\r\n",
      "Downloading polars-0.19.1-cp38-abi3-macosx_11_0_arm64.whl (16.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.6/16.6 MB\u001B[0m \u001B[31m7.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0mm\r\n",
      "\u001B[?25hDownloading protobuf-4.21.12-cp37-abi3-macosx_10_9_universal2.whl (486 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m486.2/486.2 kB\u001B[0m \u001B[31m11.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading redis-5.0.8-py3-none-any.whl (255 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m255.6/255.6 kB\u001B[0m \u001B[31m8.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.6/62.6 kB\u001B[0m \u001B[31m5.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading SQLAlchemy-1.4.53-cp312-cp312-macosx_10_9_universal2.whl (1.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m20.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0mm\r\n",
      "\u001B[?25hDownloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\r\n",
      "Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\r\n",
      "Downloading simplejson-3.19.3-cp312-cp312-macosx_11_0_arm64.whl (74 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m75.0/75.0 kB\u001B[0m \u001B[31m3.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading sqlglotrs-0.2.8-cp312-cp312-macosx_11_0_arm64.whl (290 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m290.0/290.0 kB\u001B[0m \u001B[31m12.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\r\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\r\n",
      "Downloading altair-4.2.2-py3-none-any.whl (813 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m813.6/813.6 kB\u001B[0m \u001B[31m20.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m80.9/80.9 kB\u001B[0m \u001B[31m6.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m97.9/97.9 kB\u001B[0m \u001B[31m9.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\r\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\r\n",
      "Downloading frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m51.9/51.9 kB\u001B[0m \u001B[31m4.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.7/62.7 kB\u001B[0m \u001B[31m9.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading httpcore-0.16.3-py3-none-any.whl (69 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m69.6/69.6 kB\u001B[0m \u001B[31m7.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\r\n",
      "Downloading makefun-1.15.4-py2.py3-none-any.whl (23 kB)\r\n",
      "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.3/49.3 kB\u001B[0m \u001B[31m7.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\r\n",
      "Downloading notebook-6.5.7-py3-none-any.whl (529 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m529.8/529.8 kB\u001B[0m \u001B[31m20.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading partd-1.4.2-py3-none-any.whl (18 kB)\r\n",
      "Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m85.3/85.3 kB\u001B[0m \u001B[31m11.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading PyNaCl-1.5.0-cp36-abi3-macosx_10_10_universal2.whl (349 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m349.9/349.9 kB\u001B[0m \u001B[31m12.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m103.2/103.2 kB\u001B[0m \u001B[31m13.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading python_Levenshtein-0.25.1-py3-none-any.whl (9.4 kB)\r\n",
      "Downloading Levenshtein-0.25.1-cp312-cp312-macosx_11_0_arm64.whl (103 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m103.7/103.7 kB\u001B[0m \u001B[31m9.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m240.7/240.7 kB\u001B[0m \u001B[31m3.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\r\n",
      "Downloading toolz-0.12.1-py3-none-any.whl (56 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.1/56.1 kB\u001B[0m \u001B[31m4.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading tzlocal-5.2-py3-none-any.whl (17 kB)\r\n",
      "Downloading yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m79.4/79.4 kB\u001B[0m \u001B[31m9.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\r\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\r\n",
      "Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\r\n",
      "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.6/78.6 kB\u001B[0m \u001B[31m9.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\r\n",
      "Downloading sqlglot-25.15.0-py3-none-any.whl (400 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m400.5/400.5 kB\u001B[0m \u001B[31m18.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading thefuzz-0.19.0-py2.py3-none-any.whl (17 kB)\r\n",
      "Downloading typer-0.9.0-py3-none-any.whl (45 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m45.9/45.9 kB\u001B[0m \u001B[31m4.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m87.5/87.5 kB\u001B[0m \u001B[31m7.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading nbclassic-1.1.0-py3-none-any.whl (10.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.0/10.0 MB\u001B[0m \u001B[31m22.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m0:01\u001B[0m\r\n",
      "\u001B[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\r\n",
      "Downloading wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\r\n",
      "Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\r\n",
      "Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\r\n",
      "Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\r\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\r\n",
      "Downloading rapidfuzz-3.9.6-cp312-cp312-macosx_11_0_arm64.whl (1.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.5/1.5 MB\u001B[0m \u001B[31m29.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hBuilding wheels for collected packages: newrelic\r\n",
      "  Building wheel for newrelic (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for newrelic: filename=newrelic-8.8.0-cp312-cp312-macosx_14_0_arm64.whl size=689540 sha256=4036143462a192ac7756c326d1e31e2d8662a4c68ed8d76723ef9161925b4bfe\r\n",
      "  Stored in directory: /Users/billhsieh/Library/Caches/pip/wheels/f2/45/3f/8fcdf33da08411874e19e5bfe337cdaac19fabd3efa502cb89\r\n",
      "Successfully built newrelic\r\n",
      "Installing collected packages: thefuzz, text-unidecode, rfc3986, pickleshare, makefun, ipython-genutils, backcall, wrapt, watchdog, tzlocal, typing-extensions, tornado, toolz, sqlglotrs, sqlglot, sqlalchemy, smmap, simplejson, shellingham, sentry-sdk, ruamel.yaml, requests, redis, rapidfuzz, python-dateutil, pyparsing, PyJWT, pyasn1, pyarrow, psutil, protobuf, polars, Pillow, newrelic, multidict, mdurl, marshmallow, Mako, locket, jsonpatch, Jinja2, itsdangerous, inflection, frozenlist, entrypoints, colorama, cloudpickle, Click, cachetools, bcrypt, anyio, aiohappyeyeballs, aiofiles, yarl, typer, terminado, pynacl, partd, memory-profiler, markdown-it-py, Levenshtein, ldap3, jupyter-client, httpcore, gitdb, freezegun, Faker, deprecated, datadog, cryptography, croniter, alembic, aiosignal, rich, python-levenshtein, ipython, httpx, GitPython, dask, aiohttp, PyGithub, ipykernel, altair, jupyter-server, nbclassic, notebook, great-expectations, mage_ai\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.12.2\r\n",
      "    Uninstalling typing_extensions-4.12.2:\r\n",
      "      Successfully uninstalled typing_extensions-4.12.2\r\n",
      "  Attempting uninstall: tornado\r\n",
      "    Found existing installation: tornado 6.4.1\r\n",
      "    Uninstalling tornado-6.4.1:\r\n",
      "      Successfully uninstalled tornado-6.4.1\r\n",
      "  Attempting uninstall: requests\r\n",
      "    Found existing installation: requests 2.32.3\r\n",
      "    Uninstalling requests-2.32.3:\r\n",
      "      Successfully uninstalled requests-2.32.3\r\n",
      "  Attempting uninstall: python-dateutil\r\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\r\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\r\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\r\n",
      "  Attempting uninstall: psutil\r\n",
      "    Found existing installation: psutil 6.0.0\r\n",
      "    Uninstalling psutil-6.0.0:\r\n",
      "      Successfully uninstalled psutil-6.0.0\r\n",
      "  Attempting uninstall: Pillow\r\n",
      "    Found existing installation: pillow 10.4.0\r\n",
      "    Uninstalling pillow-10.4.0:\r\n",
      "      Successfully uninstalled pillow-10.4.0\r\n",
      "  Attempting uninstall: Jinja2\r\n",
      "    Found existing installation: Jinja2 3.1.4\r\n",
      "    Uninstalling Jinja2-3.1.4:\r\n",
      "      Successfully uninstalled Jinja2-3.1.4\r\n",
      "  Attempting uninstall: anyio\r\n",
      "    Found existing installation: anyio 4.4.0\r\n",
      "    Uninstalling anyio-4.4.0:\r\n",
      "      Successfully uninstalled anyio-4.4.0\r\n",
      "  Attempting uninstall: terminado\r\n",
      "    Found existing installation: terminado 0.18.1\r\n",
      "    Uninstalling terminado-0.18.1:\r\n",
      "      Successfully uninstalled terminado-0.18.1\r\n",
      "  Attempting uninstall: jupyter-client\r\n",
      "    Found existing installation: jupyter_client 8.6.2\r\n",
      "    Uninstalling jupyter_client-8.6.2:\r\n",
      "      Successfully uninstalled jupyter_client-8.6.2\r\n",
      "  Attempting uninstall: httpcore\r\n",
      "    Found existing installation: httpcore 1.0.5\r\n",
      "    Uninstalling httpcore-1.0.5:\r\n",
      "      Successfully uninstalled httpcore-1.0.5\r\n",
      "  Attempting uninstall: ipython\r\n",
      "    Found existing installation: ipython 8.26.0\r\n",
      "    Uninstalling ipython-8.26.0:\r\n",
      "      Successfully uninstalled ipython-8.26.0\r\n",
      "  Attempting uninstall: httpx\r\n",
      "    Found existing installation: httpx 0.27.0\r\n",
      "    Uninstalling httpx-0.27.0:\r\n",
      "      Successfully uninstalled httpx-0.27.0\r\n",
      "  Attempting uninstall: ipykernel\r\n",
      "    Found existing installation: ipykernel 6.29.5\r\n",
      "    Uninstalling ipykernel-6.29.5:\r\n",
      "      Successfully uninstalled ipykernel-6.29.5\r\n",
      "  Attempting uninstall: jupyter-server\r\n",
      "    Found existing installation: jupyter_server 2.14.1\r\n",
      "    Uninstalling jupyter_server-2.14.1:\r\n",
      "      Successfully uninstalled jupyter_server-2.14.1\r\n",
      "  Attempting uninstall: notebook\r\n",
      "    Found existing installation: notebook 7.1.2\r\n",
      "    Uninstalling notebook-7.1.2:\r\n",
      "      Successfully uninstalled notebook-7.1.2\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "jupyterlab 4.1.8 requires httpx>=0.25.0, but you have httpx 0.23.1 which is incompatible.\r\n",
      "jupyterlab 4.1.8 requires jupyter-server<3,>=2.4.0, but you have jupyter-server 1.23.5 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed Click-8.1.7 Faker-4.14.0 GitPython-3.1.41 Jinja2-3.1.3 Levenshtein-0.25.1 Mako-1.3.5 Pillow-10.3.0 PyGithub-1.59.0 PyJWT-2.6.0 aiofiles-22.1.0 aiohappyeyeballs-2.4.0 aiohttp-3.10.0 aiosignal-1.3.1 alembic-1.13.2 altair-4.2.2 anyio-3.7.1 backcall-0.2.0 bcrypt-4.0.1 cachetools-5.5.0 cloudpickle-3.0.0 colorama-0.4.6 croniter-1.3.7 cryptography-41.0.6 dask-2024.8.1 datadog-0.44.0 deprecated-1.2.14 entrypoints-0.4 freezegun-1.2.2 frozenlist-1.4.1 gitdb-4.0.11 great-expectations-0.18.12 httpcore-0.16.3 httpx-0.23.1 inflection-0.5.1 ipykernel-6.15.0 ipython-8.10.0 ipython-genutils-0.2.0 itsdangerous-1.1.0 jsonpatch-1.33 jupyter-client-7.4.4 jupyter-server-1.23.5 ldap3-2.9.1 locket-1.0.0 mage_ai-0.9.73 makefun-1.15.4 markdown-it-py-3.0.0 marshmallow-3.22.0 mdurl-0.1.2 memory-profiler-0.61.0 multidict-6.0.5 nbclassic-1.1.0 newrelic-8.8.0 notebook-6.5.7 partd-1.4.2 pickleshare-0.7.5 polars-0.19.1 protobuf-4.21.12 psutil-5.9.8 pyarrow-14.0.1 pyasn1-0.6.0 pynacl-1.5.0 pyparsing-3.1.2 python-dateutil-2.8.2 python-levenshtein-0.25.1 rapidfuzz-3.9.6 redis-5.0.8 requests-2.31.0 rfc3986-1.5.0 rich-13.7.1 ruamel.yaml-0.17.17 sentry-sdk-1.19.1 shellingham-1.5.4 simplejson-3.19.3 smmap-5.0.1 sqlalchemy-1.4.53 sqlglot-25.15.0 sqlglotrs-0.2.8 terminado-0.17.1 text-unidecode-1.3 thefuzz-0.19.0 toolz-0.12.1 tornado-6.3.3 typer-0.9.0 typing-extensions-4.10.0 tzlocal-5.2 watchdog-4.0.0 wrapt-1.16.0 yarl-1.9.4\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T05:43:19.871448Z",
     "start_time": "2024-08-21T05:43:19.865799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Final code\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "from datetime import datetime\n",
    "from mage_ai.data_preparation.variable_manager import set_global_variable\n",
    "\n",
    "if 'data_exporter' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import data_exporter\n",
    "\n",
    "@data_exporter\n",
    "def elasticsearch(\n",
    "        documents: List[Dict[str, Union[Dict, List[int], np.ndarray, str]]], *args, **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Exports document data to an Elasticsearch database.\n",
    "    \"\"\"\n",
    "\n",
    "    connection_string = kwargs.get('connection_string', 'http://localhost:9200/')\n",
    "\n",
    "    # Adjusting index name\n",
    "    index_name_prefix = kwargs.get('index_name', 'documents')\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%M%S\")\n",
    "    index_name = f\"{index_name_prefix}_{current_time}\"\n",
    "    print(\"index name:\", index_name)\n",
    "\n",
    "    # Setting global variable\n",
    "    set_global_variable('resplendent_radiance', 'index_name', index_name)\n",
    "\n",
    "    number_of_shards = kwargs.get('number_of_shards', 1)\n",
    "    number_of_replicas = kwargs.get('number_of_replicas', 0)\n",
    "    vector_column_name = kwargs.get('vector_column_name', 'embedding')\n",
    "\n",
    "    dimensions = kwargs.get('dimensions')\n",
    "    if dimensions is None and len(documents) > 0:\n",
    "        document = documents[0]\n",
    "        dimensions = len(document.get(vector_column_name) or [])\n",
    "\n",
    "    es_client = Elasticsearch(connection_string)\n",
    "\n",
    "    print(f'Connecting to Elasticsearch at {connection_string}')\n",
    "\n",
    "    index_settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": number_of_shards,\n",
    "            \"number_of_replicas\": number_of_replicas\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"text\": {\"type\": \"text\"},\n",
    "                \"section\": {\"type\": \"text\"},\n",
    "                \"question\": {\"type\": \"text\"},\n",
    "                \"course\": {\"type\": \"keyword\"},\n",
    "                \"document_id\": {\"type\": \"keyword\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if not es_client.indices.exists(index=index_name):\n",
    "        es_client.indices.create(index=index_name)\n",
    "        print('Index created with properties:', index_settings)\n",
    "        print('Embedding dimensions:', dimensions)\n",
    "\n",
    "    print(f'Indexing {len(documents)} documents to Elasticsearch index {index_name}')\n",
    "    for document in processed_documents:\n",
    "        print(f'Indexing document {document[\"document_id\"]}')\n",
    "\n",
    "        es_client.index(index=index_name, document=document)\n",
    "\n",
    "    print(document)"
   ],
   "id": "dcdf41f8ca85fb00",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f782148059f97884"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
